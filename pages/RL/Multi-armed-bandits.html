<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Multi-armed Bandits – Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="Multi-armed Bandits – Learning">
<meta property="og:description" content="">
<meta property="og:site_name" content="Learning">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/RL/Multi-armed-bandits.html">Reinforcement Learning</a></li><li class="breadcrumb-item"><a href="../../pages/RL/Multi-armed-bandits.html">Multi-armed Bandits</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Learning</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Random Math</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/random/Fourier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fourier Series Representations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/random/Vectors-in-alternate-basis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vectors in Alternate Basis</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Reinforcement Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/RL/Multi-armed-bandits.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Multi-armed Bandits</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/Stats/Probability-Basics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Basic Concepts of Probability</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#summary-of-notation" id="toc-summary-of-notation" class="nav-link active" data-scroll-target="#summary-of-notation"><span class="header-section-number">1</span> Summary of Notation</a></li>
  <li><a href="#a-k-armed-bandit-problem" id="toc-a-k-armed-bandit-problem" class="nav-link" data-scroll-target="#a-k-armed-bandit-problem"><span class="header-section-number">2</span> A <span class="math inline">\(k\)</span>-armed Bandit Problem</a></li>
  <li><a href="#value-of-an-action" id="toc-value-of-an-action" class="nav-link" data-scroll-target="#value-of-an-action"><span class="header-section-number">3</span> Value of an Action</a></li>
  <li><a href="#the-problem" id="toc-the-problem" class="nav-link" data-scroll-target="#the-problem"><span class="header-section-number">4</span> The Problem</a></li>
  <li><a href="#possible-solutions" id="toc-possible-solutions" class="nav-link" data-scroll-target="#possible-solutions"><span class="header-section-number">5</span> Possible solutions</a></li>
  <li><a href="#action-value-methods" id="toc-action-value-methods" class="nav-link" data-scroll-target="#action-value-methods"><span class="header-section-number">6</span> Action-value Methods</a></li>
  <li><a href="#epsilon-greedy-methods" id="toc-epsilon-greedy-methods" class="nav-link" data-scroll-target="#epsilon-greedy-methods"><span class="header-section-number">7</span> <span class="math inline">\(\epsilon\)</span>-Greedy Methods</a></li>
  <li><a href="#softmax-action-selection" id="toc-softmax-action-selection" class="nav-link" data-scroll-target="#softmax-action-selection"><span class="header-section-number">8</span> Softmax Action Selection</a></li>
  <li><a href="#incremental-implementation" id="toc-incremental-implementation" class="nav-link" data-scroll-target="#incremental-implementation"><span class="header-section-number">9</span> Incremental Implementation</a></li>
  <li><a href="#nonstationary-problems" id="toc-nonstationary-problems" class="nav-link" data-scroll-target="#nonstationary-problems"><span class="header-section-number">10</span> Nonstationary Problems</a></li>
  <li><a href="#upper-confidence-bound-ucb-action-selection" id="toc-upper-confidence-bound-ucb-action-selection" class="nav-link" data-scroll-target="#upper-confidence-bound-ucb-action-selection"><span class="header-section-number">11</span> Upper-Confidence-Bound (UCB) Action Selection</a>
  <ul class="collapse">
  <li><a href="#ucb1-algorithm" id="toc-ucb1-algorithm" class="nav-link" data-scroll-target="#ucb1-algorithm"><span class="header-section-number">11.1</span> UCB1 Algorithm</a></li>
  <li><a href="#sec-UCB1-Theorem" id="toc-sec-UCB1-Theorem" class="nav-link" data-scroll-target="#sec-UCB1-Theorem"><span class="header-section-number">11.2</span> Theorem</a></li>
  <li><a href="#sec-Chernoff-Hoeffding-Bound" id="toc-sec-Chernoff-Hoeffding-Bound" class="nav-link" data-scroll-target="#sec-Chernoff-Hoeffding-Bound"><span class="header-section-number">11.3</span> Chernoff-Hoeffding Bound</a></li>
  <li><a href="#proof-of-ucb1-theorem" id="toc-proof-of-ucb1-theorem" class="nav-link" data-scroll-target="#proof-of-ucb1-theorem"><span class="header-section-number">11.4</span> Proof of UCB1 Theorem</a></li>
  </ul></li>
  <li><a href="#pac-bounds-for-multi-armed-bandits" id="toc-pac-bounds-for-multi-armed-bandits" class="nav-link" data-scroll-target="#pac-bounds-for-multi-armed-bandits"><span class="header-section-number">12</span> PAC Bounds for Multi-armed Bandits</a>
  <ul class="collapse">
  <li><a href="#sec-Markov-Inequality" id="toc-sec-Markov-Inequality" class="nav-link" data-scroll-target="#sec-Markov-Inequality"><span class="header-section-number">12.1</span> Markov’s Inequality</a></li>
  <li><a href="#sec-Union-Bound" id="toc-sec-Union-Bound" class="nav-link" data-scroll-target="#sec-Union-Bound"><span class="header-section-number">12.2</span> Union Bound</a></li>
  <li><a href="#naive-algorithm" id="toc-naive-algorithm" class="nav-link" data-scroll-target="#naive-algorithm"><span class="header-section-number">12.3</span> Naive Algorithm</a></li>
  <li><a href="#sec-Naive-Theorem" id="toc-sec-Naive-Theorem" class="nav-link" data-scroll-target="#sec-Naive-Theorem"><span class="header-section-number">12.4</span> Theorem</a></li>
  <li><a href="#proof" id="toc-proof" class="nav-link" data-scroll-target="#proof"><span class="header-section-number">12.5</span> Proof</a></li>
  <li><a href="#median-elimination-algorithm" id="toc-median-elimination-algorithm" class="nav-link" data-scroll-target="#median-elimination-algorithm"><span class="header-section-number">12.6</span> Median Elimination Algorithm</a></li>
  <li><a href="#sec-Median-Elimination-Theorem" id="toc-sec-Median-Elimination-Theorem" class="nav-link" data-scroll-target="#sec-Median-Elimination-Theorem"><span class="header-section-number">12.7</span> Theorem</a></li>
  <li><a href="#sec-lemma-1-Median-Elimination" id="toc-sec-lemma-1-Median-Elimination" class="nav-link" data-scroll-target="#sec-lemma-1-Median-Elimination"><span class="header-section-number">12.8</span> Lemma 1</a></li>
  <li><a href="#sec-lemma-1-proof" id="toc-sec-lemma-1-proof" class="nav-link" data-scroll-target="#sec-lemma-1-proof"><span class="header-section-number">12.9</span> Proof</a></li>
  <li><a href="#sec-lemma-2-Median-Elimination" id="toc-sec-lemma-2-Median-Elimination" class="nav-link" data-scroll-target="#sec-lemma-2-Median-Elimination"><span class="header-section-number">12.10</span> Lemma 2</a></li>
  <li><a href="#sec-lemma-2-proof" id="toc-sec-lemma-2-proof" class="nav-link" data-scroll-target="#sec-lemma-2-proof"><span class="header-section-number">12.11</span> Proof</a></li>
  <li><a href="#sec-Median-Elimination-Theorem-proof" id="toc-sec-Median-Elimination-Theorem-proof" class="nav-link" data-scroll-target="#sec-Median-Elimination-Theorem-proof"><span class="header-section-number">12.12</span> Proof of Median Elimation Theorem</a></li>
  <li><a href="#thompson-sampling" id="toc-thompson-sampling" class="nav-link" data-scroll-target="#thompson-sampling"><span class="header-section-number">12.13</span> Thompson Sampling</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/RL/Multi-armed-bandits.html">Reinforcement Learning</a></li><li class="breadcrumb-item"><a href="../../pages/RL/Multi-armed-bandits.html">Multi-armed Bandits</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Multi-armed Bandits</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Slides
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Slides for this content are available here: <a href="../../pages/RL/MABs-and-UCB1-revealjs.html">Multi-armed Bandits and Upper Confidence Bounds</a> and <a href="../../pages/RL/PAC-bounds-for-MABs-revealjs.html">PAC bounds for Multi-Armed Bandits</a>.</p>
</div>
</div>
</div>
<section id="summary-of-notation" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="summary-of-notation"><span class="header-section-number">1</span> Summary of Notation</h2>
<p>In a multi-arm bandit problem:</p>
<p><span class="math inline">\(\begin{array}{ l l l }
k &amp;  &amp; \text{number of actions (arms)}\\
t &amp;  &amp; \text{discrete time step or play number}\\
q_{*}( a) &amp;  &amp; \text{true value (expected reward) of action} \ a\\
Q_{t}( a) &amp;  &amp; \text{estimate at time} \ t\ \text{of} \ q_{*}( a)\\
N_{t}( a) &amp;  &amp; \text{number of times action} \ a\ \text{has been selected up prior to time} \ t\\
H_{t}( a) &amp;  &amp; \text{learned preference for selecting action} \ a\ \text{at time} \ t\\
\pi _{t}( a) &amp;  &amp; \text{probability of selecting action} \ a\ \text{at time} \ t\\
\overline{R}_{t} &amp;  &amp; \text{estimate at time} \ t\ \text{of expected reward given} \ \pi _{t}
\end{array}\)</span></p>
</section>
<section id="a-k-armed-bandit-problem" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="a-k-armed-bandit-problem"><span class="header-section-number">2</span> A <span class="math inline">\(k\)</span>-armed Bandit Problem</h2>
<p>You are faced repeatedly with a choice among <span class="math inline">\(k\)</span> different actions. After each choice, you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over <span class="math inline">\(1000\)</span> action selections, or <em>time steps</em>.</p>
</section>
<section id="value-of-an-action" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="value-of-an-action"><span class="header-section-number">3</span> Value of an Action</h2>
<p>The set of all actions is denoted by <span class="math inline">\(\mathcal{A}\)</span>, <span class="math inline">\(\left| \mathcal{A} \right| = k\)</span>. Each of the <span class="math inline">\(k\)</span> actions has an expected reward given the action is selected - the <em>value</em> of the action. The action selected on time step <span class="math inline">\(t\)</span> is <span class="math inline">\(A_t\)</span>. The corresponding reward is <span class="math inline">\(R_t\)</span>. The value of an arbitrary action <span class="math inline">\(a\)</span>, denoted <span class="math inline">\(q_*(a)\)</span>, is the expected reward given that <span class="math inline">\(a\)</span> is selected: <span class="math display">\[ q_*(a) = \mathbb{E}[R_t | A_t = a] \]</span></p>
</section>
<section id="the-problem" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="the-problem"><span class="header-section-number">4</span> The Problem</h2>
<p>The value of an action is unknown and must be estimated. We denote the estimated value of action <span class="math inline">\(a\)</span> at time step <span class="math inline">\(t\)</span> as <span class="math inline">\(Q_t(a)\)</span>. We would like <span class="math inline">\(Q_t(a)\)</span> to be close to <span class="math inline">\(q_*(a)\)</span>. Should we select the action with the highest estimated value or should we explore other actions? This is the exploration-exploitation dilemma/trade-off.</p>
</section>
<section id="possible-solutions" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="possible-solutions"><span class="header-section-number">5</span> Possible solutions</h2>
<ul>
<li>Identify the <em>correct</em> arm eventually
<ul>
<li>Asymptotic correctness <span class="math display">\[Q_t(a) \rightarrow q_*(a) \text{ as } t \rightarrow \infty\]</span></li>
</ul></li>
<li>Maximise the total rewards obtained. Minimize regret (loss) while learning
<ul>
<li>Regret optimality <span class="math display">\[\lim_{t \rightarrow \infty} \frac{1}{t} \sum_{i=1}^{t} (q_*(a^*) - q_*(A_t)) = 0\]</span></li>
</ul></li>
<li>Probably Approximately Correct (PAC) frameworks
<ul>
<li>Identification of an <span class="math inline">\(\epsilon\)</span>-optimal arm with probability <span class="math inline">\(1-\delta\)</span></li>
<li><span class="math inline">\(\epsilon\)</span>-Optimal: Mean of the selected arm satisfies <span class="math display">\[\mu &gt; \mu^* - \epsilon\]</span></li>
<li>Minimize sample complexity: Order of samples required for such an arm identification</li>
</ul></li>
</ul>
</section>
<section id="action-value-methods" class="level2 smaller" data-number="6">
<h2 class="smaller anchored" data-number="6" data-anchor-id="action-value-methods"><span class="header-section-number">6</span> Action-value Methods</h2>
<p>The true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received: <span class="math display">\[ Q_t(a) = \frac{\text{sum of rewards when } a \text{ taken prior to } t}{\text{number of times } a \text{ taken prior to } t} = \frac{\sum\limits_{i=1}^{t-1}R_{i}\cdot\mathbb{1}_{A_{i}=a}}{\sum\limits_{i=1}^{t-1}\mathbb{1}_{A_{i}=a}} \]</span> where <span class="math inline">\(\mathbb{1}_{\text{predicate}}\)</span> denotes the random variable that is <span class="math inline">\(1\)</span> if the predicate is true and <span class="math inline">\(0\)</span> otherwise.</p>
<p>If the denominator is <span class="math inline">\(0\)</span>, we define <span class="math inline">\(Q_t(a)\)</span> to be some default value, such as <span class="math inline">\(0\)</span>. By the law of large numbers, as <span class="math inline">\(\sum\limits_{i=1}^{t-1}\mathbb{1}_{A_{i}=a} \rightarrow \infty\)</span>, <span class="math inline">\(Q_t(a) \rightarrow q_*(a)\)</span>. This is known as the <em>sample-average</em> method for estimating action values because each estimate is an average of the sample of relevant rewards.</p>
<p>The simplest action selection rule is to select one of the actions with the highest estimated value, that is, one of the greedy actions. If there is more than one greedy action, then a selection is made among them in some arbitrary way, perhaps randomly. We write this greedy action selection method as <span class="math display">\[ A_{t} \doteq \underset{a}{\arg\max}\ Q_{t}( a) \]</span></p>
<p>Greedy action selection always exploits current knowledge to maximize immediate reward; it spends no time at all sampling apparently inferior actions to see if they might really be better.</p>
</section>
<section id="epsilon-greedy-methods" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="epsilon-greedy-methods"><span class="header-section-number">7</span> <span class="math inline">\(\epsilon\)</span>-Greedy Methods</h2>
<p>A simple alternative to allow for some exploration is to behave greedily most of the time, but every once in a while, say with small probability <span class="math inline">\(\epsilon\)</span>, instead select randomly from among all the <span class="math inline">\(k = \left| \mathcal{A} \right|\)</span> actions with equal probability, independently of the action-value estimates. We call methods using this near-greedy action selection rule <span class="math inline">\(\epsilon\)</span>-greedy methods. The <span class="math inline">\(\epsilon\)</span>-greedy action selection method is defined as follows:</p>
<ul>
<li><p>With probability <span class="math inline">\(1-\epsilon\)</span>, select <span class="math inline">\(A_t = \underset{a}{\arg\max}\ Q_t(a)\)</span> (<em>greedy</em> action)</p></li>
<li><p>With probability <span class="math inline">\(\epsilon\)</span>, select <span class="math inline">\(A_t\)</span> randomly from <span class="math inline">\(\mathcal{A}\)</span></p></li>
</ul>
<p>Whether choosing the greedy action directly or randomly choosing from all actions (including the greedy action), <span class="math display">\[ P(A_t = \underset{a}{\arg\max}\ Q_t(a)) = 1 - \epsilon + \frac{\epsilon} {k} \]</span></p>
</section>
<section id="softmax-action-selection" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="softmax-action-selection"><span class="header-section-number">8</span> Softmax Action Selection</h2>
<p>The softmax action selection rule is a “soft” version of the greedy action selection rule. In <span class="math inline">\(\epsilon\)</span>-greedy methods, the greedy action gets the most probability mass, and all the other actions have an equal probability of being selected in the exploration phase.</p>
<p>If some actions are worse than others, should try to reduce the probability of selecting them during exploration. The softmax action selection method can be denoted as <span class="math display">\[ P(A_t = a) = \frac{e^{Q_t(a)/\tau}}{\sum_{b=1}^{k}e^{Q_t(b)/\tau}} \]</span> where <span class="math inline">\(\tau\)</span> is the <em>temperature</em> parameter that controls the level of exploration. It can be “cooled” over time to reduce exploration.</p>
</section>
<section id="incremental-implementation" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="incremental-implementation"><span class="header-section-number">9</span> Incremental Implementation</h2>
<p>The action-value methods discussed so far all estimate action values as sample averages of observed rewards. To simplify notation, we focus on a single action. Let <span class="math inline">\(R_i\)</span> now denote the reward received after the <span class="math inline">\(i\)</span>th selection <em>of this action</em>, and let <span class="math inline">\(Q_n\)</span> denote the estimate of its action value after it has been selected <span class="math inline">\(n-1\)</span> times, which we can now write simply as <span class="math display">\[ Q_{n} \doteq \frac{R_{1} + R_{2} + \ldots + R_{n-1}}{n-1} \]</span> The obvious implementation would be to maintain a record of all the rewards and then perform this computation whenever the estimated value was needed. However, if all the rewards are stored, as more rewards are seen, the memory and computational requirements would keep increasing. These averages can be computed in a computationally efficient manner, in particular, with constant memory and constant per-time-step computation.</p>
<p>Given <span class="math inline">\(Q_{n}\)</span> and the <span class="math inline">\(n\)</span>th reward, <span class="math inline">\(R_{n}\)</span>, the new average of all <span class="math inline">\(n\)</span> rewards can be computed by</p>
<p><span class="math display">\[\begin{aligned}
Q_{n+1} &amp; =\frac{1}{n} \ \sum\limits_{i=1}^{n} R_{i}\\
&amp; =\frac{1}{n} \ \left( R_{n} +\sum\limits_{i=1}^{n-1} R_{i}\right)\\
&amp; =\frac{1}{n} \ \left( R_{n} +( n-1)\frac{1}{n-1} \ \sum\limits _{i=1}^{n-1} R_{i}\right)\\
&amp; =\frac{1}{n} \ ( R_{n} +( n-1) Q_{n})\\
&amp; =\frac{1}{n} \ ( R_{n} +nQ_{n} -Q_{n})\\
&amp; =Q_{n} +\frac{1}{n}[ R_{n} -Q_{n}]
\end{aligned}\]</span> which holds even for <span class="math inline">\(n=1\)</span>, obtaining <span class="math inline">\(Q_{2} = R_{1}\)</span> for arbitrary <span class="math inline">\(Q_{1}\)</span>.</p>
<p>This update rule has the form of a stochastic averaging equation. The general form of such an equation is <span class="math display">\[ NewEstimate \leftarrow OldEstimate + StepSize \left[ Target - OldEstimate \right] \]</span></p>
<p>The expression <span class="math inline">\([Target - OldEstimate]\)</span> is an error in the estimate. It is reduced by taking a step toward the “Target”. The step-size parameter <span class="math inline">\((StepSize)\)</span> used in the incremental implementation changes from time step to time step. In processing the <span class="math inline">\(n\)</span>th reward for an action, the step-size parameter is <span class="math inline">\(\frac{1}{n}\)</span>. It is often denoted by <span class="math inline">\(\alpha\)</span> or, more generally, by <span class="math inline">\(\alpha_{t}(a)\)</span>.</p>
</section>
<section id="nonstationary-problems" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="nonstationary-problems"><span class="header-section-number">10</span> Nonstationary Problems</h2>
<p>The methods discussed so far are appropriate for stationary bandit problems, where the reward probabilities do not change over time. In nonstationary cases, it makes sense to give more weight to recent rewards than to long-past rewards.</p>
<p>One way to do this is to use a constant step-size parameter. The incremental update rule for updating an average <span class="math inline">\(Q_{n}\)</span> of the <span class="math inline">\(n-1\)</span> past rewards is modified to be <span class="math display">\[ Q_{n+1} = Q_{n} + \alpha \left[ R_{n} - Q_{n} \right] \]</span> where the step-size parameter <span class="math inline">\(\alpha \in (0, 1]\)</span> is constant.</p>
<p>This results in <span class="math inline">\(Q_{n+1}\)</span> being a weighted average of past rewards and the initial estimate <span class="math inline">\(Q_{1}\)</span>: <span class="math display">\[\begin{array}{ r c l }
Q_{n+1} &amp; = &amp; Q_{n} +\alpha [R_{n} -Q_{n} ]\\
&amp; = &amp; \alpha R_{n} +(1-\alpha )Q_{n}\\
&amp; = &amp; \alpha R_{n} +(1-\alpha )[\alpha R_{n-1} +(1-\alpha )Q_{n-1} ]\\
&amp; = &amp; \alpha R_{n} +(1-\alpha )\alpha R_{n-1} +(1-\alpha )^{2} Q_{n-1}\\
&amp; = &amp; \alpha R_{n} +(1-\alpha )\alpha R_{n-1} +(1-\alpha )^{2} \alpha R_{n-2} +\\
&amp;  &amp; \dotsc +(1-\alpha )^{n-1} \alpha R_{1} +(1-\alpha )^{n} Q_{1}\\
&amp; = &amp; \ (1-\alpha )^{n} Q_{1} +\sum\limits _{i=1}^{n} \alpha (1-\alpha )^{n-i} R_{i}
\end{array}\]</span></p>
<p>We call this a weighted average because the sum of the weights is <span class="math inline">\((1-\alpha )^{n} +\sum\limits _{i=1}^{n} \alpha (1-\alpha )^{n-i} = 1\)</span>.</p>
<p>Note that the weight <span class="math inline">\(\alpha (1-\alpha )^{n-i}\)</span>, given to the reward <span class="math inline">\(R_{i}\)</span>, depends on how many rewards ago, <span class="math inline">\(n-i\)</span>, it was observed. The quantity <span class="math inline">\(1-\alpha\)</span> is less than <span class="math inline">\(1\)</span>, so the weight given to <span class="math inline">\(R_i\)</span> decreases as the number of intervening rewards increases. In fact, the weight decays exponentially according to the exponent on <span class="math inline">\(1-\alpha\)</span>. If <span class="math inline">\(1-\alpha = 0\)</span>, then all the weight goes on the very last reward, <span class="math inline">\(R_n\)</span>, because of the convention that <span class="math inline">\(0^0 = 1\)</span>. Accordingly, this is sometimes called an <em>exponential recency-weighted average</em>.</p>
</section>
<section id="upper-confidence-bound-ucb-action-selection" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="upper-confidence-bound-ucb-action-selection"><span class="header-section-number">11</span> Upper-Confidence-Bound (UCB) Action Selection</h2>
<p>In its most basic formulation, a <span class="math inline">\(K\)</span>-armed bandit problem is defined by random variables <span class="math inline">\(X_{i,n}\)</span> for <span class="math inline">\(1 \leq i \leq K\)</span> and <span class="math inline">\(n \geq 1\)</span>, where <span class="math inline">\(i\)</span> is the index of a gambling machine (i.e., the “arm” of the bandit). Successive plays of arm <span class="math inline">\(i\)</span> yield rewards <span class="math inline">\(X_{i,1}, X_{i,2}, \ldots\)</span> that are independent and identically distributed (i.i.d.) random variables to an unknown law with unknown expectation <span class="math inline">\(\mu_i\)</span>. Independence also holds for rewards across different arms; i.e., <span class="math inline">\(X_{i,s}\)</span> and <span class="math inline">\(X_{j,t}\)</span> are independent (and usually not identically distributed) for each <span class="math inline">\(1 \leq i &lt; j \leq K\)</span> and each <span class="math inline">\(s, t \geq 1\)</span>.</p>
<p>A <em>policy</em>, or <em>allocation strategy</em>, <span class="math inline">\(A\)</span> is an algorithm that chooses the next arm to play based on the sequence of past plays and obtained rewards. Let <span class="math inline">\(T_{i}(n)\)</span> be the random variable which represents the number of times arm <span class="math inline">\(i\)</span> has been played by <span class="math inline">\(A\)</span> during the first <span class="math inline">\(n\)</span> plays. Then the <em>regret</em> of <span class="math inline">\(A\)</span> after <span class="math inline">\(n\)</span> plays is defined by <span class="math display">\[\mu^* n - \mu_{j} \sum\limits_{i=1}^{K} \mathbb{E}[T_{j}(n)]\]</span> where <span class="math inline">\(\mu^* = \underset{1\leq i \leq K}{\max} \mu_i\)</span> and <span class="math inline">\(\mathbb{E}[\cdot]\)</span> denotes the expectation. Thus the regret is the expected loss due to the fact that the policy does not always play the best arm.</p>
<section id="ucb1-algorithm" class="level3" data-number="11.1">
<h3 data-number="11.1" class="anchored" data-anchor-id="ucb1-algorithm"><span class="header-section-number">11.1</span> UCB1 Algorithm</h3>
<div class="line-block"><strong>Deterministic policy</strong>: UCB1.<br>
<strong>Initialization</strong>: Play each arm once.<br>
<strong>Loop</strong>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Play arm <span class="math inline">\(j\)</span> that maximizes <span class="math inline">\(\bar{x}_j + \sqrt{\frac{2\ln n}{n_j}}\)</span>,</div>
<p>where <span class="math inline">\(\bar{x}_j\)</span> is the average reward obtained from arm <span class="math inline">\(j\)</span>, <span class="math inline">\(n_j\)</span> is the number of times arm <span class="math inline">\(j\)</span> has been played so far, and <span class="math inline">\(n\)</span> is the overall number of plays so far.</p>
</section>
<section id="sec-UCB1-Theorem" class="level3" data-number="11.2">
<h3 data-number="11.2" class="anchored" data-anchor-id="sec-UCB1-Theorem"><span class="header-section-number">11.2</span> Theorem</h3>
<p>For all <span class="math inline">\(K &gt; 1\)</span>, if policy UCB1 is run on <span class="math inline">\(K\)</span> arms having arbitrary reward distributions <span class="math inline">\(P_1,\dots,P_{K}\)</span> with support in <span class="math inline">\([0, 1]\)</span>, then its expected regret after any number of plays <span class="math inline">\(n\)</span> is at most <span class="math display">\[ \left[8 \sum\limits_{i:\mu_i &lt; \mu^*} \left( \frac{\ln n}{\Delta_{i}} \right) \right] + \left(1+ \frac{\pi^2}{3}  \right) \left(\sum\limits_{j=1}^{K} \Delta_{j} \right)\]</span> where <span class="math inline">\(\mu_1,\dots,\mu_K\)</span> are the expected values of <span class="math inline">\(P_1,\dots,P_{K}\)</span>.</p>
<p>Matching the notation of the theorem from the original paper to our notation, we get:</p>
<ul>
<li><span class="math inline">\(\bar{x}_j = Q_t(j)\)</span></li>
<li><span class="math inline">\(\mu_i = \mathbb{E}[X_{i,n}] = q_*(i)\)</span></li>
<li><span class="math inline">\(\mu^* = q_*(a^*)\)</span></li>
<li><span class="math inline">\(\Delta_{i} = \mu^* - \mu_i = q_*(a^*) - q_*(i)\)</span></li>
<li><span class="math inline">\(\text{Regret}_{n}=\sum\limits_{i} \mathbb{E}[T_{i}(n)] \Delta_{i}\)</span></li>
</ul>
<p>To prove the theorem, we need to show that, for any suboptimal arm <span class="math inline">\(j\)</span>, <span class="math display">\[\mathbb{E}[T_{j}(n)] \leq \frac{8 \ln n}{\Delta_{j}^2}+c\]</span></p>
<p>We also define the r.v.’s <span class="math inline">\(I_1, I_2, \ldots\)</span> such that $I_t denotes the arm played at time <span class="math inline">\(t\)</span>. Also, <span class="math inline">\(\sum_{i=1}^{K} T_{i}(n) = n\)</span>. For each <span class="math inline">\(1\leq i \leq K\)</span>, and <span class="math inline">\(n \geq 1\)</span>, define <span class="math display">\[\bar{X}_{i,n} = \frac{1}{n} \sum\limits_{t=1}^{n} X_{i,t} \]</span></p>
</section>
<section id="sec-Chernoff-Hoeffding-Bound" class="level3" data-number="11.3">
<h3 data-number="11.3" class="anchored" data-anchor-id="sec-Chernoff-Hoeffding-Bound"><span class="header-section-number">11.3</span> Chernoff-Hoeffding Bound</h3>
<p>Let <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> be random variables with common range <span class="math inline">\([0, 1]\)</span> and such that <span class="math inline">\(\mathbb{E}[X_t|X_1, X_2, \ldots, X_{t-1}] = \mu\)</span>. Let <span class="math inline">\(S_n = \frac{X_1 + X_2 + \ldots + X_n}{n}\)</span>. Then, for all <span class="math inline">\(\epsilon &gt; 0\)</span>,</p>
<p><span class="math display">\[ P(S_n \geq \mu + \epsilon) \leq e^{-2\epsilon^{2}n} \text{ and } P(S_n \leq \mu - \epsilon) \leq e^{-2\epsilon^{2}n} \]</span></p>
</section>
<section id="proof-of-ucb1-theorem" class="level3" data-number="11.4">
<h3 data-number="11.4" class="anchored" data-anchor-id="proof-of-ucb1-theorem"><span class="header-section-number">11.4</span> Proof of UCB1 Theorem</h3>
<p>Let <span class="math inline">\(c_{t,s} = \sqrt{\frac{2\ln t}{s}}\)</span>. For any arm <span class="math inline">\(i\)</span>, we upper bound <span class="math inline">\(T_{i}(n)\)</span> on any sequence of plays. More precisely, for each <span class="math inline">\(t \geq 1\)</span> we bound the indicator function of <span class="math inline">\(I_t=i\)</span> as follows. Let <span class="math inline">\(\mathscr{l}\)</span> be an arbitrary positive integer. Note: <span class="math inline">\(\displaystyle \{I_{t} =i\} =1\)</span> if arm <span class="math inline">\(i\)</span> is played at time <span class="math inline">\(t\)</span> and <span class="math inline">\(0\)</span> otherwise.</p>
<span class="math display">\[\begin{aligned}
T_i(n) &amp;= 1 + \sum\limits_{t=K+1}^{n} \{I_{t} =i\} \\
&amp;\leq \mathscr{l} + \sum\limits_{t=K+1}^{n} \{I_{t} =i, T_{i}(t-1) \geq \mathscr{l} \} \\
&amp;\leq \mathscr{l} + \sum\limits_{t=K+1}^{n} \{\bar{X}_{T^{*}(t-1)}^{*} + c_{t-1,T^{*}(t-1)} \leq \bar{X}_{i,T_{i}(t-1)} + c_{t-1,T_{i}(t-1)}, T_{i}(t-1) \geq \mathscr{l} \}  \\
&amp;\leq \mathscr{l} + \sum\limits_{t=K+1}^{n} \{ \underset{0&lt;s&lt;t}{\min} \bar{X}_{s}^{*} + c_{t-1,s} \leq \underset{\mathscr{l}\ \leq s_i &lt; t}{\max} \bar{X}_{i,s_i} + c_{t-1,s_i} \} \\

&amp;\leq \mathscr{l} + \sum\limits_{t=1}^{\infty} \sum\limits_{s=1}^{t-1} \sum\limits_{s_{i}=\mathscr{l}}^{t-1} \{ \bar{X}_{s}^{*} + c_{t,s} \leq \bar{X}_{i, s_{i}} + c_{t,s_i} \}
\end{aligned}\]</span>
<p>Now observe that <span class="math inline">\(\bar{X}_{s}^{*} + c_{t,s} \leq \bar{X}_{i, s_{i}} + c_{t,s_i}\)</span> implies that at least one of the following must hold</p>
<ol type="1">
<li><span class="math inline">\(\overline{X}_{s}^{*} +c_{t,s} \leqslant \ \mu ^{*} -c_{t,s}\)</span></li>
<li><span class="math inline">\(\overline{X}_{i,s_{i}} \geqslant \mu _{i} +c_{t,s_{i}}\)</span></li>
<li><span class="math inline">\(\mu ^{*} &lt; \mu _{i} +2c_{t,s_{i}}\)</span></li>
</ol>
<p>We bound the probability of the first two events using the Chernoff-Hoeffding bound as shown in <a href="#sec-Chernoff-Hoeffding-Bound" class="quarto-xref">Section&nbsp;11.3</a>.</p>
<p><span class="math inline">\(P(\overline{X}_{s}^{*} +c_{t,s} \leqslant \ \mu ^{*} -c_{t,s}) \leq e^{-2c_{t,s}^{2}s} = e^{-4\ln t} = t^{-4}\)</span></p>
<p><span class="math inline">\(P(\overline{X}_{i,s_{i}} \geqslant \mu _{i} +c_{t,s_{i}}) \leq e^{-2c_{t,s_{i}}^{2}s_{i}} = e^{-4\ln t} = t^{-4}\)</span></p>
<p>For <span class="math inline">\(\mathscr{l} =\left\lceil \frac{8\ \ln n}{\Delta _{i}^{2}}\right\rceil\)</span>, the third event is impossible. In fact</p>
<p><span class="math inline">\(\mu ^{*} - \mu _{i} - 2c_{t,s_{i}} = \mu ^{*} - \mu _{i} - 2\sqrt{\frac{2\ln t}{s_{i}}} \geq \mu ^{*} - \mu _{i} - \Delta _{i} = \Delta _{i} - \Delta _{i} = 0\)</span></p>
<p>for <span class="math inline">\(s_i \geq \frac{8\ln n}{\Delta_i^2}\)</span>. So we get</p>
<span class="math display">\[\begin{array}{ c c l }
\mathbb{E}[ T_{i}( n)] &amp; \leqslant  &amp;  \begin{array}{l}
\left\lceil \frac{8\ \ln n}{\Delta _{i}^{2}}\right\rceil +\sum\limits _{t=1}^{\infty }\sum\limits _{s=1}^{t-1}\sum\limits _{s_{i} =\left\lceil \frac{8\ \ln n}{\Delta _{i}^{2}}\right\rceil }^{t-1}\\
\times \ \left( P\left\{\overline{X}_{s}^{*} \leqslant \mu ^{*} -c_{t,s}\right\} +P\{\overline{X}_{i,s_{i}} \geqslant \mu _{i} +c_{t,s_{i}}\}\right)
\end{array}\\
&amp; \leqslant  &amp; \left\lceil \frac{8\ \ln n}{\Delta _{i}^{2}}\right\rceil +\sum\limits _{t=1}^{\infty }\sum\limits _{s=1}^{t}\sum\limits _{s_{i} =1}^{t} 2t^{-4}\\
&amp; \leqslant  &amp; \frac{8\ \ln n}{\Delta _{i}^{2}} +1+\frac{\pi ^{2}}{3}
\end{array}\]</span>
<p>which concludes the proof.</p>
</section>
</section>
<section id="pac-bounds-for-multi-armed-bandits" class="level2" data-number="12">
<h2 data-number="12" class="anchored" data-anchor-id="pac-bounds-for-multi-armed-bandits"><span class="header-section-number">12</span> PAC Bounds for Multi-armed Bandits</h2>
<section id="sec-Markov-Inequality" class="level3" data-number="12.1">
<h3 data-number="12.1" class="anchored" data-anchor-id="sec-Markov-Inequality"><span class="header-section-number">12.1</span> Markov’s Inequality</h3>
<p>Let <span class="math inline">\(X\)</span> be a non-negative random variable. Then, for any <span class="math inline">\(a &gt; 0\)</span>, <span class="math display">\[ P(X \geq a) \leq \frac{\mathbb{E}[X]}{a} \]</span></p>
</section>
<section id="sec-Union-Bound" class="level3" data-number="12.2">
<h3 data-number="12.2" class="anchored" data-anchor-id="sec-Union-Bound"><span class="header-section-number">12.2</span> Union Bound</h3>
<p>Let <span class="math inline">\(A_1, A_2, \ldots, A_n\)</span> be events. Then, <span class="math display">\[ P(A_1 \cup A_2 \cup \ldots \cup A_n) \leq P(A_1) + P(A_2) + \ldots + P(A_n) \]</span> <span class="math display">\[ \equiv P\left( \bigcup_{i=1}^{n} A_i \right) \leq \sum_{i=1}^{n} P(A_i) \]</span></p>
</section>
<section id="naive-algorithm" class="level3" data-number="12.3">
<h3 data-number="12.3" class="anchored" data-anchor-id="naive-algorithm"><span class="header-section-number">12.3</span> Naive Algorithm</h3>
<div class="line-block"><strong>Input</strong>: <span class="math inline">\(\epsilon &gt; 0, \delta &gt;0\)</span><br>
<strong>Output</strong>: An arm<br>
<strong>foreach</strong> Arm <span class="math inline">\(a \in \mathcal{A}\)</span> <strong>do</strong><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sample it <span class="math inline">\(\mathscr{l} = \frac{2}{\epsilon^2} \ln (\frac{2k}{\delta})\)</span> times;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Let <span class="math inline">\(\hat{p}_a\)</span> be the average reward of arm <span class="math inline">\(a\)</span>;<br>
<strong>end</strong><br>
Output <span class="math inline">\(a' = \underset{a \in \mathcal{A}}{\arg\max} \{ \hat{p}_a \}\)</span>;</div>
</section>
<section id="sec-Naive-Theorem" class="level3" data-number="12.4">
<h3 data-number="12.4" class="anchored" data-anchor-id="sec-Naive-Theorem"><span class="header-section-number">12.4</span> Theorem</h3>
<p>The algorithm Naive(<span class="math inline">\(\epsilon, \delta\)</span>) is an (<span class="math inline">\(\epsilon, \delta\)</span>)-PAC algorithm with arm sample complexity <span class="math inline">\(O((\frac{k}{\epsilon^2}) \log (\frac{k}{\delta}))\)</span>.</p>
</section>
<section id="proof" class="level3" data-number="12.5">
<h3 data-number="12.5" class="anchored" data-anchor-id="proof"><span class="header-section-number">12.5</span> Proof</h3>
<p>Let <span class="math inline">\(a'\)</span> be an arm s.t. <span class="math inline">\(q_*(a')&lt;q_*(a^*)-\epsilon\)</span></p>
<p><span class="math inline">\(P(Q(a')&gt;Q(a^*)) \leq P(Q(a')&gt;q_*(a')+ \frac{\epsilon} {2} \text{ or } Q(a^*)&lt;q_*(a^*)- \frac{\epsilon} {2})\)</span></p>
<span class="math display">\[\begin{aligned}
P(Q(a')&gt;Q(a^*)) &amp;\leq P(Q(a')&gt;q_*(a')+ \frac{\epsilon}{2} \text{ or } Q(a^*)&lt;q_*(a^*)- \frac{\epsilon}{2}) \\
&amp;\leq P(Q(a')&gt;q_*(a')+ \frac{\epsilon}{2}) + P(Q(a^*)&lt;q_*(a^*)- \frac{\epsilon}{2}) \\
&amp;\leq 2\exp(-2\left( \frac{\epsilon}{2} \right)^2 \mathscr{l})
\end{aligned}\]</span>
<p>Substituting <span class="math inline">\(\mathscr{l} = \frac{2}{\epsilon^2} \ln (\frac{2k}{\delta})\)</span>,</p>
<p><span class="math display">\[P(Q(a')&gt;Q(a^*)) \leq 2\exp(-2\left( \frac{\epsilon}{2} \right)^2 \frac{2}{\epsilon^2} \ln (\frac{2k}{\delta}))\]</span> <span class="math display">\[\implies P(Q(a')&gt;Q(a^*)) \leq 2\exp(- \frac{2}{2} \ln (\frac{2k}{\delta}))\]</span> <span class="math display">\[\implies P(Q(a')&gt;Q(a^*)) \leq 2\exp(\ln(\frac{2k}{\delta})^{-1})\]</span> <span class="math display">\[\implies P(Q(a')&gt;Q(a^*)) \leq \frac{\delta}{k}\]</span></p>
<p>Summing over all <span class="math inline">\(a'\)</span>, we have that <span class="math display">\[\text{probability of failure} \leq (k-1)(\frac{\delta}{k}) &lt; \delta\]</span></p>
</section>
<section id="median-elimination-algorithm" class="level3" data-number="12.6">
<h3 data-number="12.6" class="anchored" data-anchor-id="median-elimination-algorithm"><span class="header-section-number">12.6</span> Median Elimination Algorithm</h3>
<div class="line-block"><strong>Input</strong>: <span class="math inline">\(\epsilon &gt; 0, \delta &gt;0\)</span><br>
<strong>Output</strong>: An arm<br>
Set <span class="math inline">\(S_1=A, \epsilon_1=\frac{\epsilon}{4}, \delta_1=\frac{\delta}{2}, \mathscr{l} = 1\)</span>. <strong>repeat</strong><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sample every arm <span class="math inline">\(a \in S_{\mathscr{l}}\)</span> for <span class="math inline">\(\frac{1}{(\epsilon_{\mathscr{l}})^2/2} \cdot \log \left( \frac{3}{\delta _{\mathscr{l}}} \right)\)</span> times, and let <span class="math inline">\(\hat{p}_a^{\mathscr{l}}\)</span> denote its empirical value;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Find the median of <span class="math inline">\(\hat{p}_a^{\mathscr{l}}\)</span>, denoted by <span class="math inline">\(m_{\mathscr{l}}\)</span>;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(S_{\mathscr{l}+1} = S_{\mathscr{l}} \backslash \{ a : \hat{p}_a^{\mathscr{l}} &lt; m_{\mathscr{l}} \}\)</span>;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\epsilon_{\mathscr{l}+1} = \frac{3}{4} \epsilon_{\mathscr{l}}\)</span>; <span class="math inline">\(\delta_{\mathscr{l}+1} = \frac{\delta_{\mathscr{l}}}{2}\)</span>; <span class="math inline">\(\mathscr{l} = \mathscr{l} + 1\)</span>;<br>
<strong>until</strong> <span class="math inline">\(\left|S_{\mathscr{l}}\right| = 1\)</span>;</div>
<p>Matching the notation used in the paper to our notation: <span class="math inline">\(\hat{p}_a^{\mathscr{l}} = Q_{\mathscr{l}}(a)\)</span></p>
</section>
<section id="sec-Median-Elimination-Theorem" class="level3" data-number="12.7">
<h3 data-number="12.7" class="anchored" data-anchor-id="sec-Median-Elimination-Theorem"><span class="header-section-number">12.7</span> Theorem</h3>
<p>The Median Elimination (<span class="math inline">\(\epsilon, \delta\)</span>) algorithm is an (<span class="math inline">\(\epsilon, \delta\)</span>)-PAC algorithm and its sample complexity is</p>
<p><span class="math display">\[ O \left( \frac{k}{\epsilon^2} \log \left(\frac{1}{\delta} \right) \right) \]</span></p>
<p>First we show that in the <span class="math inline">\(\mathscr{l}\)</span>-th phase the expected reward of the best arm in <span class="math inline">\(S_{\mathscr{l}}\)</span> drops by at most <span class="math inline">\(\epsilon_{\mathscr{l}}\)</span>.</p>
</section>
<section id="sec-lemma-1-Median-Elimination" class="level3" data-number="12.8">
<h3 data-number="12.8" class="anchored" data-anchor-id="sec-lemma-1-Median-Elimination"><span class="header-section-number">12.8</span> Lemma 1</h3>
<p>For the Median Elimination (<span class="math inline">\(\epsilon, \delta\)</span>) algorithm we have that for every phase <span class="math inline">\(\mathscr{l}\)</span>: <span class="math display">\[ P \left[ \underset{j \in S_{\mathscr{l}}}{\max} p_{j} \leq \underset{i \in S_{\mathscr{l}+1}}{\max} p_{i} + \epsilon_{\mathscr{l}} \right] \geq 1 - \delta_{\mathscr{l}} \]</span></p>
<p><span class="math display">\[ \equiv P \left[ \underset{j \in S_{\mathscr{l}}}{\max} q_{*}(j) \leq \underset{i \in S_{\mathscr{l}+1}}{\max} q_{*}(i) + \epsilon_{\mathscr{l}} \right] \geq 1 - \delta_{\mathscr{l}} \]</span></p>
</section>
<section id="sec-lemma-1-proof" class="level3" data-number="12.9">
<h3 data-number="12.9" class="anchored" data-anchor-id="sec-lemma-1-proof"><span class="header-section-number">12.9</span> Proof</h3>
<p>Without loss of generality consider <span class="math inline">\(\mathscr{l}=1\)</span>. We bound the failure probability by looking at the event <span class="math inline">\(E_1\)</span>,</p>
<p><span class="math display">\[ E_1 = \left\{ \hat{p}_1 &lt; p_1 - \frac{\epsilon_1}{2} \right\} \equiv E_1 = \left\{ Q(a_{\mathscr{l}}^*) &lt; q_*(a_{\mathscr{l}}^*)- \frac{\epsilon_1}{2} \right\} \]</span></p>
<p>which is the case that the empirical estimate of the best arm is pessimistic. Since we sample sufficiently, we have that</p>
<p><span class="math display">\[ P[E_1] = P\left[Q(a_{\mathscr{l}}^*) &lt; q_*(a_{\mathscr{l}}^*)- \frac{\epsilon_1}{2}\right]\]</span></p>
<p>Applying the Chernoff-Hoeffding bound (<a href="#sec-Chernoff-Hoeffding-Bound" class="quarto-xref">Section&nbsp;11.3</a>), we have that</p>
<p><span class="math display">\[ P[E_1] \leq \exp \left(-2 \left( \frac{\epsilon_1}{2} \right)^2 \frac{1}{(\epsilon_{1})^2/2} \cdot \log \left( \frac{3}{\delta _{1}} \right) \right) \]</span></p>
<p><span class="math display">\[ \implies P[E_1] \leq \frac{\delta_1}{3} \]</span></p>
<p>In case <span class="math inline">\(E_1\)</span> does not happen, we calculate the probability that an arm <span class="math inline">\(j\)</span> which is not an <span class="math inline">\(\epsilon_1\)</span>-optimal arm is empirically better than the best arm.</p>
<p><span class="math display">\[ P \left[ Q_{\mathscr{l}}(j) \geq Q_{\mathscr{l}}(a_{\mathscr{l}}^*) | Q_{\mathscr{l}}(a_{\mathscr{l}}^*) \geq q_{*}(a_{\mathscr{l}}^*) - \frac{\epsilon_1}{2} \right] \leq \frac{\delta_1}{3}\]</span></p>
<p>Let #bad be the number of arms that are not <span class="math inline">\(\epsilon_1\)</span>-optimal but are empirically better than the best arm. We have that</p>
<p><span class="math display">\[ \mathbb{E} \left[ \# bad \middle| Q_{\mathscr{l}}(a_{\mathscr{l}}^*) \geq q_{*}(a_{\mathscr{l}}^*) - \frac{\epsilon_1}{2} \right] \leq |S_\mathscr{l}| \frac{ \delta_{1}} {3} \]</span></p>
<p>Next we apply Markov’s inequality (<a href="#sec-Markov-Inequality" class="quarto-xref">Section&nbsp;12.1</a>) to obtain</p>
<p><span class="math display">\[ P \left [ \# bad \geq \frac{|S_\mathscr{l}|}{2} \middle| Q_{\mathscr{l}}(a_{\mathscr{l}}^*) \geq q_{*}(a_{\mathscr{l}}^*) - \frac{\epsilon_1}{2} \right] \leq \frac{|S_\mathscr{l}|\frac{\delta_1}{3}}{\frac{|S_\mathscr{l}|}{2}} = \frac{2\delta_{1}}{3} \]</span></p>
<p>Using the union bound (<a href="#sec-Union-Bound" class="quarto-xref">Section&nbsp;12.2</a>) gives us that the probability of failure is bounded by <span class="math inline">\(\delta_1\)</span>.</p>
</section>
<section id="sec-lemma-2-Median-Elimination" class="level3" data-number="12.10">
<h3 data-number="12.10" class="anchored" data-anchor-id="sec-lemma-2-Median-Elimination"><span class="header-section-number">12.10</span> Lemma 2</h3>
<p>The sample complexity of the Median Elimination <span class="math inline">\((\epsilon, \delta)\)</span> is <span class="math inline">\(O \left( \frac{n}{\epsilon^2} \log \left(\frac{1}{\delta} \right) \right)\)</span>.</p>
</section>
<section id="sec-lemma-2-proof" class="level3" data-number="12.11">
<h3 data-number="12.11" class="anchored" data-anchor-id="sec-lemma-2-proof"><span class="header-section-number">12.11</span> Proof</h3>
<p>The number of arm samples in the <span class="math inline">\(\mathscr{l}\)</span>-th round is <span class="math inline">\(4n_{\mathscr{l}} log  (\frac{3}{\delta_{\mathscr{l}}} ) / \epsilon_{\mathscr{l}}^2\)</span>. By definition we have that</p>
<ol type="1">
<li><span class="math inline">\(\delta_{1} = \frac{\delta}{2}\)</span> ; <span class="math inline">\(\frac{\delta_{\mathscr{l}-1}}{2}= \frac{\delta}{2^{\mathscr{l}}}\)</span></li>
<li><span class="math inline">\(n_1 = n\)</span> ; <span class="math inline">\(n_{\mathscr{l}} = n_{\mathscr{l}-1}/2 = n/2^{\mathscr{l}-1}\)</span></li>
<li><span class="math inline">\(\epsilon_{1} = \frac{\epsilon}{4}\)</span> ; <span class="math inline">\(\epsilon_{\mathscr{l}} = \frac{3}{4} \epsilon_{\mathscr{l}-1} = ( \frac{3}{4} )^{\mathscr{l}-1} \epsilon / 4\)</span></li>
</ol>
<p>Therefore we have</p>
<span class="math display">\[\begin{aligned}
\sum\limits_{\mathscr{l} =1}^{\log_2 n} \frac{n_{\mathscr{l} } \log  (3 / \delta_{\mathscr{l} } )} {(\epsilon_{\mathscr{l} }/2)^2} &amp;= 4 \sum\limits_{\mathscr{l} =1}^{\log_2 n} \frac{n/2^{\mathscr{l} -1} \log (2^{\mathscr{l} }3/ \delta)}{((\frac{3}{4})^{\mathscr{l} -1} \epsilon /4)^2} \\
&amp;= 64 \sum\limits_{\mathscr{l} =1}^{\log_2 n} n(\frac{8}{9})^{\mathscr{l} -1} (\frac{\log(1/\delta)}{\epsilon^2} + \frac{\log (3)}{\epsilon^2} + \frac{\mathscr{l} \log (2)}{\epsilon^2}) \\
&amp;\leq 64 \frac{n \log(1/\delta)}{\epsilon^2} \sum\limits_{\mathscr{l} =1}^{\infty} (\frac{8}{9})^{\mathscr{l} -1} (\mathscr{l}C' + C) = O \left( \frac{n \log (1/\delta)}{\epsilon^2} \right)
\end{aligned}\]</span>
<p>Now the <a href="#sec-Median-Elimination-Theorem" class="quarto-xref">Section&nbsp;12.7</a> can be proved.</p>
</section>
<section id="sec-Median-Elimination-Theorem-proof" class="level3" data-number="12.12">
<h3 data-number="12.12" class="anchored" data-anchor-id="sec-Median-Elimination-Theorem-proof"><span class="header-section-number">12.12</span> Proof of Median Elimation Theorem</h3>
<p>From <a href="#sec-lemma-2-Median-Elimination" class="quarto-xref">Section&nbsp;12.10</a> we have that the sample complexity is bounded by <span class="math inline">\(O \left( n \log (1/\delta) / \epsilon^2 \right)\)</span>. By <a href="#sec-lemma-1-Median-Elimination" class="quarto-xref">Section&nbsp;12.8</a> we have that the algorithm fails with probability _{i} in each round so that over all rounds the probability of failure is bounded by <span class="math inline">\(\sum_{i=1}^{\log_2 n} \delta_{i} \leq \delta\)</span>. In each round we reduce the optimal reward of the surviving arms by at most <span class="math inline">\(\epsilon_{i}\)</span> so that the total error is bounded by <span class="math inline">\(\sum_{i=1}^{\log_2 n} \epsilon_{i} \leq \epsilon\)</span>.</p>
</section>
<section id="thompson-sampling" class="level3" data-number="12.13">
<h3 data-number="12.13" class="anchored" data-anchor-id="thompson-sampling"><span class="header-section-number">12.13</span> Thompson Sampling</h3>
<p>Update computations can be very complex, but for certain distributions (called <em>conjugate priors</em>) they are easy. One possibility is to select actions at each step according to their posterior probability of being the best action. This method, sometimes called <em>posterior sampling</em> or <em>Thompson sampling</em>, often performs similarly to the bets of the distribution-free methods.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/arachnidly\.github\.io\/learning\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>