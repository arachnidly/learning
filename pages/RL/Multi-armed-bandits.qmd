---
title: "Multi-armed Bandits"
format:
  html: 
    toc: true
    toc_depth: 2
    smooth-scroll: true
    number-sections: true
    crossrefs-hover: true
    html-table-processing: none
---

::: {.callout-tip collapse="true"}
## Slides

Slides for this content are available here: [Multi-armed Bandits and Upper Confidence Bounds](MABs-and-UCB1-revealjs.qmd) and [PAC bounds for Multi-Armed Bandits](PAC-bounds-for-MABs-revealjs.qmd).
:::

## Summary of Notation

In a multi-arm bandit problem:

$\begin{array}{ l l l }
k &  & \text{number of actions (arms)}\\
t &  & \text{discrete time step or play number}\\
q_{*}( a) &  & \text{true value (expected reward) of action} \ a\\
Q_{t}( a) &  & \text{estimate at time} \ t\ \text{of} \ q_{*}( a)\\
N_{t}( a) &  & \text{number of times action} \ a\ \text{has been selected up prior to time} \ t\\
H_{t}( a) &  & \text{learned preference for selecting action} \ a\ \text{at time} \ t\\
\pi _{t}( a) &  & \text{probability of selecting action} \ a\ \text{at time} \ t\\
\overline{R}_{t} &  & \text{estimate at time} \ t\ \text{of expected reward given} \ \pi _{t}
\end{array}$

## A $k$-armed Bandit Problem

You are faced repeatedly with a choice among $k$ different actions. After each choice, you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over $1000$ action selections, or *time steps*.

## Value of an Action

The set of all actions is denoted by $\mathcal{A}$, $\left| \mathcal{A} \right| = k$. Each of the $k$ actions has an expected reward given the action is selected - the *value* of the action. The action selected on time step $t$ is $A_t$. The corresponding reward is $R_t$. The value of an arbitrary action $a$, denoted $q_*(a)$, is the expected reward given that $a$ is selected: $$ q_*(a) = \mathbb{E}[R_t | A_t = a] $$

## The Problem

The value of an action is unknown and must be estimated. We denote the estimated value of action $a$ at time step $t$ as $Q_t(a)$. We would like $Q_t(a)$ to be close to $q_*(a)$. Should we select the action with the highest estimated value or should we explore other actions? This is the exploration-exploitation dilemma/trade-off.

## Possible solutions

-   Identify the *correct* arm eventually
    -   Asymptotic correctness $$Q_t(a) \rightarrow q_*(a) \text{ as } t \rightarrow \infty$$
-   Maximise the total rewards obtained. Minimize regret (loss) while learning
    -   Regret optimality $$\lim_{t \rightarrow \infty} \frac{1}{t} \sum_{i=1}^{t} (q_*(a^*) - q_*(A_t)) = 0$$
-   Probably Approximately Correct (PAC) frameworks
    -   Identification of an $\epsilon$-optimal arm with probability $1-\delta$
    -   $\epsilon$-Optimal: Mean of the selected arm satisfies $$\mu > \mu^* - \epsilon$$
    -   Minimize sample complexity: Order of samples required for such an arm identification

## Action-value Methods

The true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received: $$ Q_t(a) = \frac{\text{sum of rewards when } a \text{ taken prior to } t}{\text{number of times } a \text{ taken prior to } t} = \frac{\sum\limits_{i=1}^{t-1}R_{i}\cdot\mathbb{1}_{A_{i}=a}}{\sum\limits_{i=1}^{t-1}\mathbb{1}_{A_{i}=a}} $$ where $\mathbb{1}_{\text{predicate}}$ denotes the random variable that is $1$ if the predicate is true and $0$ otherwise.

If the denominator is $0$, we define $Q_t(a)$ to be some default value, such as $0$. By the law of large numbers, as $\sum\limits_{i=1}^{t-1}\mathbb{1}_{A_{i}=a} \rightarrow \infty$, $Q_t(a) \rightarrow q_*(a)$. This is known as the *sample-average* method for estimating action values because each estimate is an average of the sample of relevant rewards.

The simplest action selection rule is to select one of the actions with the highest estimated value, that is, one of the greedy actions. If there is more than one greedy action, then a selection is made among them in some arbitrary way, perhaps randomly. We write this greedy action selection method as $$ A_{t} \doteq \underset{a}{\arg\max}\ Q_{t}( a) $$

Greedy action selection always exploits current knowledge to maximize immediate reward; it spends no time at all sampling apparently inferior actions to see if they might really be better.

## $\epsilon$-Greedy Methods

A simple alternative to allow for some exploration is to behave greedily most of the time, but every once in a while, say with small probability $\epsilon$, instead select randomly from among all the $k = \left| \mathcal{A} \right|$ actions with equal probability, independently of the action-value estimates. We call methods using this near-greedy action selection rule $\epsilon$-greedy methods. The $\epsilon$-greedy action selection method is defined as follows:

-   With probability $1-\epsilon$, select $A_t = \underset{a}{\arg\max}\ Q_t(a)$ (*greedy* action)

-   With probability $\epsilon$, select $A_t$ randomly from $\mathcal{A}$

Whether choosing the greedy action directly or randomly choosing from all actions (including the greedy action), $$ P(A_t = \underset{a}{\arg\max}\ Q_t(a)) = 1 - \epsilon + \frac{\epsilon} {k} $$

## Softmax Action Selection

The softmax action selection rule is a "soft" version of the greedy action selection rule. In $\epsilon$-greedy methods, the greedy action gets the most probability mass, and all the other actions have an equal probability of being selected in the exploration phase.

If some actions are worse than others, should try to reduce the probability of selecting them during exploration. The softmax action selection method can be denoted as $$ P(A_t = a) = \frac{e^{Q_t(a)/\tau}}{\sum_{b=1}^{k}e^{Q_t(b)/\tau}} $$ where $\tau$ is the *temperature* parameter that controls the level of exploration. It can be "cooled" over time to reduce exploration.

## Incremental Implementation

The action-value methods discussed so far all estimate action values as sample averages of observed rewards. To simplify notation, we focus on a single action. Let $R_i$ now denote the reward received after the $i$th selection *of this action*, and let $Q_n$ denote the estimate of its action value after it has been selected $n-1$ times, which we can now write simply as $$ Q_{n} \doteq \frac{R_{1} + R_{2} + \ldots + R_{n-1}}{n-1} $$ The obvious implementation would be to maintain a record of all the rewards and then perform this computation whenever the estimated value was needed. However, if all the rewards are stored, as more rewards are seen, the memory and computational requirements would keep increasing. These averages can be computed in a computationally efficient manner, in particular, with constant memory and constant per-time-step computation.

Given $Q_{n}$ and the $n$th reward, $R_{n}$, the new average of all $n$ rewards can be computed by

$$\begin{aligned}
Q_{n+1} & =\frac{1}{n} \ \sum\limits_{i=1}^{n} R_{i}\\
 & =\frac{1}{n} \ \left( R_{n} +\sum\limits_{i=1}^{n-1} R_{i}\right)\\
 & =\frac{1}{n} \ \left( R_{n} +( n-1)\frac{1}{n-1} \ \sum\limits _{i=1}^{n-1} R_{i}\right)\\
 & =\frac{1}{n} \ ( R_{n} +( n-1) Q_{n})\\
 & =\frac{1}{n} \ ( R_{n} +nQ_{n} -Q_{n})\\
 & =Q_{n} +\frac{1}{n}[ R_{n} -Q_{n}]
\end{aligned}$$ which holds even for $n=1$, obtaining $Q_{2} = R_{1}$ for arbitrary $Q_{1}$.

This update rule has the form of a stochastic averaging equation. The general form of such an equation is $$ NewEstimate \leftarrow OldEstimate + StepSize \left[ Target - OldEstimate \right] $$

The expression $[Target - OldEstimate]$ is an error in the estimate. It is reduced by taking a step toward the "Target". The step-size parameter $(StepSize)$ used in the incremental implementation changes from time step to time step. In processing the $n$th reward for an action, the step-size parameter is $\frac{1}{n}$. It is often denoted by $\alpha$ or, more generally, by $\alpha_{t}(a)$.

## Nonstationary Problems

The methods discussed so far are appropriate for stationary bandit problems, where the reward probabilities do not change over time. In nonstationary cases, it makes sense to give more weight to recent rewards than to long-past rewards.

One way to do this is to use a constant step-size parameter. The incremental update rule for updating an average $Q_{n}$ of the $n-1$ past rewards is modified to be $$ Q_{n+1} = Q_{n} + \alpha \left[ R_{n} - Q_{n} \right] $$ where the step-size parameter $\alpha \in (0, 1]$ is constant.

This results in $Q_{n+1}$ being a weighted average of past rewards and the initial estimate $Q_{1}$: $$\begin{array}{ r c l }
Q_{n+1} & = & Q_{n} +\alpha [R_{n} -Q_{n} ]\\
 & = & \alpha R_{n} +(1-\alpha )Q_{n}\\
 & = & \alpha R_{n} +(1-\alpha )[\alpha R_{n-1} +(1-\alpha )Q_{n-1} ]\\
 & = & \alpha R_{n} +(1-\alpha )\alpha R_{n-1} +(1-\alpha )^{2} Q_{n-1}\\
 & = & \alpha R_{n} +(1-\alpha )\alpha R_{n-1} +(1-\alpha )^{2} \alpha R_{n-2} +\\
 &  & \dotsc +(1-\alpha )^{n-1} \alpha R_{1} +(1-\alpha )^{n} Q_{1}\\
 & = & \ (1-\alpha )^{n} Q_{1} +\sum\limits _{i=1}^{n} \alpha (1-\alpha )^{n-i} R_{i}
\end{array}$$

We call this a weighted average because the sum of the weights is $(1-\alpha )^{n} +\sum\limits _{i=1}^{n} \alpha (1-\alpha )^{n-i} = 1$.

Note that the weight $\alpha (1-\alpha )^{n-i}$, given to the reward $R_{i}$, depends on how many rewards ago, $n-i$, it was observed. The quantity $1-\alpha$ is less than $1$, so the weight given to $R_i$ decreases as the number of intervening rewards increases. In fact, the weight decays exponentially according to the exponent on $1-\alpha$. If $1-\alpha = 0$, then all the weight goes on the very last reward, $R_n$, because of the convention that $0^0 = 1$. Accordingly, this is sometimes called an *exponential recency-weighted average*.

## Upper-Confidence-Bound (UCB) Action Selection

In its most basic formulation, a $K$-armed bandit problem is defined by random variables $X_{i,n}$ for $1 \leq i \leq K$ and $n \geq 1$, where $i$ is the index of a gambling machine (i.e., the "arm" of the bandit). Successive plays of arm $i$ yield rewards $X_{i,1}, X_{i,2}, \ldots$ that are independent and identically distributed (i.i.d.) random variables to an unknown law with unknown expectation $\mu_i$. Independence also holds for rewards across different arms; i.e., $X_{i,s}$ and $X_{j,t}$ are independent (and usually not identically distributed) for each $1 \leq i < j \leq K$ and each $s, t \geq 1$.

A *policy*, or *allocation strategy*, $A$ is an algorithm that chooses the next arm to play based on the sequence of past plays and obtained rewards. Let $T_{i}(n)$ be the random variable which represents the number of times arm $i$ has been played by $A$ during the first $n$ plays. Then the *regret* of $A$ after $n$ plays is defined by $$\mu^* n - \mu_{j} \sum\limits_{i=1}^{K} \mathbb{E}[T_{j}(n)]$$ where $\mu^* = \underset{1\leq i \leq K}{\max} \mu_i$ and $\mathbb{E}[\cdot]$ denotes the expectation. Thus the regret is the expected loss due to the fact that the policy does not always play the best arm.

### UCB1 Algorithm

| **Deterministic policy**: UCB1.
| **Initialization**: Play each arm once.
| **Loop**:
|             Play arm $j$ that maximizes $\bar{x}_j + \sqrt{\frac{2\ln n}{n_j}}$,

where $\bar{x}_j$ is the average reward obtained from arm $j$, $n_j$ is the number of times arm $j$ has been played so far, and $n$ is the overall number of plays so far.

### Theorem {#sec-UCB1-Theorem}

For all $K > 1$, if policy UCB1 is run on $K$ arms having arbitrary reward distributions $P_1,\dots,P_{K}$ with support in $[0, 1]$, then its expected regret after any number of plays $n$ is at most $$ \left[8 \sum\limits_{i:\mu_i < \mu^*} \left( \frac{\ln n}{\Delta_{i}} \right) \right] + \left(1+ \frac{\pi^2}{3}  \right) \left(\sum\limits_{j=1}^{K} \Delta_{j} \right)$$ where $\mu_1,\dots,\mu_K$ are the expected values of $P_1,\dots,P_{K}$.

Matching the notation of the theorem from the original paper to our notation, we get:

-   $\bar{x}_j = Q_t(j)$
-   $\mu_i = \mathbb{E}[X_{i,n}] = q_*(i)$
-   $\mu^* = q_*(a^*)$
-   $\Delta_{i} = \mu^* - \mu_i = q_*(a^*) - q_*(i)$
-   $\text{Regret}_{n}=\sum\limits_{i} \mathbb{E}[T_{i}(n)] \Delta_{i}$

To prove the theorem, we need to show that, for any suboptimal arm $j$, $$\mathbb{E}[T_{j}(n)] \leq \frac{8 \ln n}{\Delta_{j}^2}+c$$

We also define the r.v.'s $I_1, I_2, \ldots$ such that \$I_t denotes the arm played at time $t$. Also, $\sum_{i=1}^{K} T_{i}(n) = n$. For each $1\leq i \leq K$, and $n \geq 1$, define $$\bar{X}_{i,n} = \frac{1}{n} \sum\limits_{t=1}^{n} X_{i,t} $$

### Chernoff-Hoeffding Bound {#sec-Chernoff-Hoeffding-Bound}

Let $X_1, X_2, \ldots, X_n$ be random variables with common range $[0, 1]$ and such that $\mathbb{E}[X_t|X_1, X_2, \ldots, X_{t-1}] = \mu$. Let $S_n = \frac{X_1 + X_2 + \ldots + X_n}{n}$. Then, for all $\epsilon > 0$,

$$ P(S_n \geq \mu + \epsilon) \leq e^{-2\epsilon^{2}n} \text{ and } P(S_n \leq \mu - \epsilon) \leq e^{-2\epsilon^{2}n} $$

### Proof of UCB1 Theorem

Let $c_{t,s} = \sqrt{\frac{2\ln t}{s}}$. For any arm $i$, we upper bound $T_{i}(n)$ on any sequence of plays. More precisely, for each $t \geq 1$ we bound the indicator function of $I_t=i$ as follows. Let $\mathscr{l}$ be an arbitrary positive integer. Note: $\displaystyle \{I_{t} =i\} =1$ if arm $i$ is played at time $t$ and $0$ otherwise.

```{=tex}
\begin{aligned}
T_i(n) &= 1 + \sum\limits_{t=K+1}^{n} \{I_{t} =i\} \\
&\leq \mathscr{l} + \sum\limits_{t=K+1}^{n} \{I_{t} =i, T_{i}(t-1) \geq \mathscr{l} \} \\
&\leq \mathscr{l} + \sum\limits_{t=K+1}^{n} \{\bar{X}_{T^{*}(t-1)}^{*} + c_{t-1,T^{*}(t-1)} \leq \bar{X}_{i,T_{i}(t-1)} + c_{t-1,T_{i}(t-1)}, T_{i}(t-1) \geq \mathscr{l} \}  \\
&\leq \mathscr{l} + \sum\limits_{t=K+1}^{n} \{ \underset{0<s<t}{\min} \bar{X}_{s}^{*} + c_{t-1,s} \leq \underset{\mathscr{l}\ \leq s_i < t}{\max} \bar{X}_{i,s_i} + c_{t-1,s_i} \} \\

&\leq \mathscr{l} + \sum\limits_{t=1}^{\infty} \sum\limits_{s=1}^{t-1} \sum\limits_{s_{i}=\mathscr{l}}^{t-1} \{ \bar{X}_{s}^{*} + c_{t,s} \leq \bar{X}_{i, s_{i}} + c_{t,s_i} \}
\end{aligned}
```
Now observe that $\bar{X}_{s}^{*} + c_{t,s} \leq \bar{X}_{i, s_{i}} + c_{t,s_i}$ implies that at least one of the following must hold

1.  $\overline{X}_{s}^{*} +c_{t,s} \leqslant \ \mu ^{*} -c_{t,s}$
2.  $\overline{X}_{i,s_{i}} \geqslant \mu _{i} +c_{t,s_{i}}$
3.  $\mu ^{*} < \mu _{i} +2c_{t,s_{i}}$

We bound the probability of the first two events using the Chernoff-Hoeffding bound as shown in @sec-Chernoff-Hoeffding-Bound.

$P(\overline{X}_{s}^{*} +c_{t,s} \leqslant \ \mu ^{*} -c_{t,s}) \leq e^{-2c_{t,s}^{2}s} = e^{-4\ln t} = t^{-4}$

$P(\overline{X}_{i,s_{i}} \geqslant \mu _{i} +c_{t,s_{i}}) \leq e^{-2c_{t,s_{i}}^{2}s_{i}} = e^{-4\ln t} = t^{-4}$

For $\mathscr{l} =\left\lceil \frac{8\ \ln n}{\Delta _{i}^{2}}\right\rceil$, the third event is impossible. In fact

$\mu ^{*} - \mu _{i} - 2c_{t,s_{i}} = \mu ^{*} - \mu _{i} - 2\sqrt{\frac{2\ln t}{s_{i}}} \geq \mu ^{*} - \mu _{i} - \Delta _{i} = \Delta _{i} - \Delta _{i} = 0$

for $s_i \geq \frac{8\ln n}{\Delta_i^2}$. So we get

```{=tex}
\begin{array}{ c c l }
\mathbb{E}[ T_{i}( n)] & \leqslant  &  \begin{array}{l}
\left\lceil \frac{8\ \ln n}{\Delta _{i}^{2}}\right\rceil +\sum\limits _{t=1}^{\infty }\sum\limits _{s=1}^{t-1}\sum\limits _{s_{i} =\left\lceil \frac{8\ \ln n}{\Delta _{i}^{2}}\right\rceil }^{t-1}\\
\times \ \left( P\left\{\overline{X}_{s}^{*} \leqslant \mu ^{*} -c_{t,s}\right\} +P\{\overline{X}_{i,s_{i}} \geqslant \mu _{i} +c_{t,s_{i}}\}\right)
\end{array}\\
 & \leqslant  & \left\lceil \frac{8\ \ln n}{\Delta _{i}^{2}}\right\rceil +\sum\limits _{t=1}^{\infty }\sum\limits _{s=1}^{t}\sum\limits _{s_{i} =1}^{t} 2t^{-4}\\
 & \leqslant  & \frac{8\ \ln n}{\Delta _{i}^{2}} +1+\frac{\pi ^{2}}{3}
\end{array}
```
which concludes the proof.

## PAC Bounds for Multi-armed Bandits

### Markov's Inequality {#sec-Markov-Inequality}

Let $X$ be a non-negative random variable. Then, for any $a > 0$, $$ P(X \geq a) \leq \frac{\mathbb{E}[X]}{a} $$

### Union Bound {#sec-Union-Bound}

Let $A_1, A_2, \ldots, A_n$ be events. Then, $$ P(A_1 \cup A_2 \cup \ldots \cup A_n) \leq P(A_1) + P(A_2) + \ldots + P(A_n) $$ $$ \equiv P\left( \bigcup_{i=1}^{n} A_i \right) \leq \sum_{i=1}^{n} P(A_i) $$

### Naive Algorithm

| **Input**: $\epsilon > 0, \delta >0$
| **Output**: An arm
| **foreach** Arm $a \in \mathcal{A}$ **do**
|         Sample it $\mathscr{l} = \frac{2}{\epsilon^2} \ln (\frac{2k}{\delta})$ times;
|         Let $\hat{p}_a$ be the average reward of arm $a$;
| **end**
| Output $a' = \underset{a \in \mathcal{A}}{\arg\max} \{ \hat{p}_a \}$;

### Theorem {#sec-Naive-Theorem}

The algorithm Naive($\epsilon, \delta$) is an ($\epsilon, \delta$)-PAC algorithm with arm sample complexity $O((\frac{k}{\epsilon^2}) \log (\frac{k}{\delta}))$.

### Proof

Let $a'$ be an arm s.t. $q_*(a')<q_*(a^*)-\epsilon$

$P(Q(a')>Q(a^*)) \leq P(Q(a')>q_*(a')+ \frac{\epsilon} {2} \text{ or } Q(a^*)<q_*(a^*)- \frac{\epsilon} {2})$

```{=tex}
\begin{aligned}
P(Q(a')>Q(a^*)) &\leq P(Q(a')>q_*(a')+ \frac{\epsilon}{2} \text{ or } Q(a^*)<q_*(a^*)- \frac{\epsilon}{2}) \\
&\leq P(Q(a')>q_*(a')+ \frac{\epsilon}{2}) + P(Q(a^*)<q_*(a^*)- \frac{\epsilon}{2}) \\
&\leq 2\exp(-2\left( \frac{\epsilon}{2} \right)^2 \mathscr{l})
\end{aligned}
```
Substituting $\mathscr{l} = \frac{2}{\epsilon^2} \ln (\frac{2k}{\delta})$,

$$P(Q(a')>Q(a^*)) \leq 2\exp(-2\left( \frac{\epsilon}{2} \right)^2 \frac{2}{\epsilon^2} \ln (\frac{2k}{\delta}))$$ $$\implies P(Q(a')>Q(a^*)) \leq 2\exp(- \frac{2}{2} \ln (\frac{2k}{\delta}))$$ $$\implies P(Q(a')>Q(a^*)) \leq 2\exp(\ln(\frac{2k}{\delta})^{-1})$$ $$\implies P(Q(a')>Q(a^*)) \leq \frac{\delta}{k}$$

Summing over all $a'$, we have that $$\text{probability of failure} \leq (k-1)(\frac{\delta}{k}) < \delta$$

### Median Elimination Algorithm

| **Input**: $\epsilon > 0, \delta >0$
| **Output**: An arm
| Set $S_1=A, \epsilon_1=\frac{\epsilon}{4}, \delta_1=\frac{\delta}{2}, \mathscr{l} = 1$. **repeat**
|         Sample every arm $a \in S_{\mathscr{l}}$ for $\frac{1}{(\epsilon_{\mathscr{l}})^2/2} \cdot \log \left( \frac{3}{\delta _{\mathscr{l}}} \right)$ times, and let $\hat{p}_a^{\mathscr{l}}$ denote its empirical value;
|         Find the median of $\hat{p}_a^{\mathscr{l}}$, denoted by $m_{\mathscr{l}}$;
|         $S_{\mathscr{l}+1} = S_{\mathscr{l}} \backslash \{ a : \hat{p}_a^{\mathscr{l}} < m_{\mathscr{l}} \}$;
|         $\epsilon_{\mathscr{l}+1} = \frac{3}{4} \epsilon_{\mathscr{l}}$; $\delta_{\mathscr{l}+1} = \frac{\delta_{\mathscr{l}}}{2}$; $\mathscr{l} = \mathscr{l} + 1$;
| **until** $\left|S_{\mathscr{l}}\right| = 1$;

Matching the notation used in the paper to our notation: $\hat{p}_a^{\mathscr{l}} = Q_{\mathscr{l}}(a)$

### Theorem {#sec-Median-Elimination-Theorem}

The Median Elimination ($\epsilon, \delta$) algorithm is an ($\epsilon, \delta$)-PAC algorithm and its sample complexity is

$$ O \left( \frac{k}{\epsilon^2} \log \left(\frac{1}{\delta} \right) \right) $$

First we show that in the $\mathscr{l}$-th phase the expected reward of the best arm in $S_{\mathscr{l}}$ drops by at most $\epsilon_{\mathscr{l}}$.

### Lemma 1 {#sec-lemma-1-Median-Elimination}

For the Median Elimination ($\epsilon, \delta$) algorithm we have that for every phase $\mathscr{l}$: $$ P \left[ \underset{j \in S_{\mathscr{l}}}{\max} p_{j} \leq \underset{i \in S_{\mathscr{l}+1}}{\max} p_{i} + \epsilon_{\mathscr{l}} \right] \geq 1 - \delta_{\mathscr{l}} $$

$$ \equiv P \left[ \underset{j \in S_{\mathscr{l}}}{\max} q_{*}(j) \leq \underset{i \in S_{\mathscr{l}+1}}{\max} q_{*}(i) + \epsilon_{\mathscr{l}} \right] \geq 1 - \delta_{\mathscr{l}} $$

### Proof {#sec-lemma-1-proof}

Without loss of generality consider $\mathscr{l}=1$. We bound the failure probability by looking at the event $E_1$,

$$ E_1 = \left\{ \hat{p}_1 < p_1 - \frac{\epsilon_1}{2} \right\} \equiv E_1 = \left\{ Q(a_{\mathscr{l}}^*) < q_*(a_{\mathscr{l}}^*)- \frac{\epsilon_1}{2} \right\} $$

which is the case that the empirical estimate of the best arm is pessimistic. Since we sample sufficiently, we have that

$$ P[E_1] = P\left[Q(a_{\mathscr{l}}^*) < q_*(a_{\mathscr{l}}^*)- \frac{\epsilon_1}{2}\right]$$

Applying the Chernoff-Hoeffding bound (@sec-Chernoff-Hoeffding-Bound), we have that

$$ P[E_1] \leq \exp \left(-2 \left( \frac{\epsilon_1}{2} \right)^2 \frac{1}{(\epsilon_{1})^2/2} \cdot \log \left( \frac{3}{\delta _{1}} \right) \right) $$

$$ \implies P[E_1] \leq \frac{\delta_1}{3} $$

In case $E_1$ does not happen, we calculate the probability that an arm $j$ which is not an $\epsilon_1$-optimal arm is empirically better than the best arm.

$$ P \left[ Q_{\mathscr{l}}(j) \geq Q_{\mathscr{l}}(a_{\mathscr{l}}^*) | Q_{\mathscr{l}}(a_{\mathscr{l}}^*) \geq q_{*}(a_{\mathscr{l}}^*) - \frac{\epsilon_1}{2} \right] \leq \frac{\delta_1}{3}$$

Let #bad be the number of arms that are not $\epsilon_1$-optimal but are empirically better than the best arm. We have that

$$ \mathbb{E} \left[ \# bad \middle| Q_{\mathscr{l}}(a_{\mathscr{l}}^*) \geq q_{*}(a_{\mathscr{l}}^*) - \frac{\epsilon_1}{2} \right] \leq |S_\mathscr{l}| \frac{ \delta_{1}} {3} $$

Next we apply Markov's inequality (@sec-Markov-Inequality) to obtain

$$ P \left [ \# bad \geq \frac{|S_\mathscr{l}|}{2} \middle| Q_{\mathscr{l}}(a_{\mathscr{l}}^*) \geq q_{*}(a_{\mathscr{l}}^*) - \frac{\epsilon_1}{2} \right] \leq \frac{|S_\mathscr{l}|\frac{\delta_1}{3}}{\frac{|S_\mathscr{l}|}{2}} = \frac{2\delta_{1}}{3} $$

Using the union bound (@sec-Union-Bound) gives us that the probability of failure is bounded by $\delta_1$.

### Lemma 2 {#sec-lemma-2-Median-Elimination}

The sample complexity of the Median Elimination $(\epsilon, \delta)$ is $O \left( \frac{n}{\epsilon^2} \log \left(\frac{1}{\delta} \right) \right)$.

### Proof {#sec-lemma-2-proof}

The number of arm samples in the $\mathscr{l}$-th round is $4n_{\mathscr{l}} log  (\frac{3}{\delta_{\mathscr{l}}} ) / \epsilon_{\mathscr{l}}^2$. By definition we have that

1.  $\delta_{1} = \frac{\delta}{2}$ ; $\frac{\delta_{\mathscr{l}-1}}{2}= \frac{\delta}{2^{\mathscr{l}}}$
2.  $n_1 = n$ ; $n_{\mathscr{l}} = n_{\mathscr{l}-1}/2 = n/2^{\mathscr{l}-1}$
3.  $\epsilon_{1} = \frac{\epsilon}{4}$ ; $\epsilon_{\mathscr{l}} = \frac{3}{4} \epsilon_{\mathscr{l}-1} = ( \frac{3}{4} )^{\mathscr{l}-1} \epsilon / 4$

Therefore we have

```{=tex}
\begin{aligned}
\sum\limits_{\mathscr{l} =1}^{\log_2 n} \frac{n_{\mathscr{l} } \log  (3 / \delta_{\mathscr{l} } )} {(\epsilon_{\mathscr{l} }/2)^2} &= 4 \sum\limits_{\mathscr{l} =1}^{\log_2 n} \frac{n/2^{\mathscr{l} -1} \log (2^{\mathscr{l} }3/ \delta)}{((\frac{3}{4})^{\mathscr{l} -1} \epsilon /4)^2} \\
&= 64 \sum\limits_{\mathscr{l} =1}^{\log_2 n} n(\frac{8}{9})^{\mathscr{l} -1} (\frac{\log(1/\delta)}{\epsilon^2} + \frac{\log (3)}{\epsilon^2} + \frac{\mathscr{l} \log (2)}{\epsilon^2}) \\
&\leq 64 \frac{n \log(1/\delta)}{\epsilon^2} \sum\limits_{\mathscr{l} =1}^{\infty} (\frac{8}{9})^{\mathscr{l} -1} (\mathscr{l}C' + C) = O \left( \frac{n \log (1/\delta)}{\epsilon^2} \right)
\end{aligned}
```
Now the @sec-Median-Elimination-Theorem can be proved.

### Proof of Median Elimation Theorem {#sec-Median-Elimination-Theorem-proof}

From @sec-lemma-2-Median-Elimination we have that the sample complexity is bounded by $O \left( n \log (1/\delta) / \epsilon^2 \right)$. By @sec-lemma-1-Median-Elimination we have that the algorithm fails with probability \delta\_{i} in each round so that over all rounds the probability of failure is bounded by $\sum_{i=1}^{\log_2 n} \delta_{i} \leq \delta$. In each round we reduce the optimal reward of the surviving arms by at most $\epsilon_{i}$ so that the total error is bounded by $\sum_{i=1}^{\log_2 n} \epsilon_{i} \leq \epsilon$.

### Thompson Sampling

Update computations can be very complex, but for certain distributions (called *conjugate priors*) they are easy. One possibility is to select actions at each step according to their posterior probability of being the best action. This method, sometimes called *posterior sampling* or *Thompson sampling*, often performs similarly to the bets of the distribution-free methods.