<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.5.57">

  <title>Learning :) – Multi-armed Bandits and Upper Confidence Bounds</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="Multi-armed Bandits and Upper Confidence Bounds – Learning :)">
<meta property="og:site_name" content="Learning :)">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Multi-armed Bandits and Upper Confidence Bounds</h1>

<div class="quarto-title-authors">
</div>

</section>
<section id="a-k-armed-bandit-problem" class="slide level2">
<h2>A <span class="math inline">\(k\)</span>-armed Bandit Problem</h2>
<div class="fragment">
<p>You are faced repeatedly with a choice among <span class="math inline">\(k\)</span> different actions.</p>
</div>
<div class="fragment">
<p>After each choice, you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected.</p>
</div>
<div class="fragment">
<p>Your objective is to maximize the expected total reward over some time period, for example, over <span class="math inline">\(1000\)</span> action selections, or <em>time steps</em>.</p>
</div>
</section>
<section id="value-of-an-action" class="slide level2">
<h2>Value of an Action</h2>
<div class="fragment">
<p>The set of all actions is denoted by <span class="math inline">\(\mathcal{A}\)</span>, <span class="math inline">\(\left| \mathcal{A} \right| = k\)</span>.</p>
</div>
<div class="fragment">
<p>Each of the <span class="math inline">\(k\)</span> actions has an expected reward given the action is selected - the <em>value</em> of the action.</p>
</div>
<div class="fragment">
<p>The action selected on time step <span class="math inline">\(t\)</span> is <span class="math inline">\(A_t\)</span>.</p>
</div>
<div class="fragment">
<p>The corresponding reward is <span class="math inline">\(R_t\)</span>.</p>
</div>
<div class="fragment">
<p>The value of an arbitrary action <span class="math inline">\(a\)</span>, denoted <span class="math inline">\(q_*(a)\)</span>, is the expected reward given that <span class="math inline">\(a\)</span> is selected:</p>
</div>
<div class="fragment">
<p><span class="math display">\[ q_*(a) = \mathbb{E}[R_t | A_t = a] \]</span></p>
</div>
</section>
<section id="the-problem" class="slide level2">
<h2>The Problem</h2>
<div class="fragment">
<p>The value of an action is unknown and must be estimated.</p>
</div>
<div class="fragment">
<p>We denote the estimated value of action <span class="math inline">\(a\)</span> at time step <span class="math inline">\(t\)</span> as <span class="math inline">\(Q_t(a)\)</span>.</p>
</div>
<div class="fragment">
<p>We would like <span class="math inline">\(Q_t(a)\)</span> to be close to <span class="math inline">\(q_*(a)\)</span>.</p>
</div>
<div class="fragment">
<p>Should we select the action with the highest estimated value or should we explore other actions? This is the exploration-exploitation dilemma/trade-off.</p>
</div>
</section>
<section id="possible-solutions" class="slide level2">
<h2>Possible solutions</h2>
<ul>
<li class="fragment">Asymptotic correctness</li>
</ul>
<div class="fragment">
<p><span class="math display">\[Q_t(a) \rightarrow q_*(a) \text{ as } t \rightarrow \infty\]</span></p>
</div>
<ul>
<li class="fragment">Regret optimality</li>
</ul>
<div class="fragment">
<p><span class="math display">\[\lim_{t \rightarrow \infty} \frac{1}{t} \sum_{i=1}^{t} (q_*(a^*) - q_*(A_t)) = 0\]</span></p>
</div>
<ul>
<li class="fragment">PAC (Probably Approximately Correct) optimality</li>
</ul>
<div class="fragment">
<p><span class="math display">\[P(q_*(A_t) \geq q_*(a^*) - \epsilon) \geq (1-\delta)\]</span></p>
</div>
</section>
<section id="action-value-methods" class="slide level2 smaller">
<h2>Action-value Methods</h2>
<div class="fragment">
<p>The true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received:</p>
</div>
<div class="fragment">
<p><span class="math display">\[ Q_t(a) = \frac{\text{sum of rewards when } a \text{ taken prior to } t}{\text{number of times } a \text{ taken prior to } t} = \frac{\sum\limits_{i=1}^{t-1}R_{i}\cdot\mathbb{1}_{A_{i}=a}}{\sum\limits_{i=1}^{t-1}\mathbb{1}_{A_{i}=a}} \]</span></p>
</div>
<div class="fragment">
<p>where <span class="math inline">\(\mathbb{1}_{\text{predicate}}\)</span> denotes the random variable that is <span class="math inline">\(1\)</span> if the predicate is true and <span class="math inline">\(0\)</span> otherwise.</p>
</div>
<div class="fragment">
<p>If the denominator is <span class="math inline">\(0\)</span>, we define <span class="math inline">\(Q_t(a)\)</span> to be some default value, such as <span class="math inline">\(0\)</span>.</p>
</div>
<div class="fragment">
<p>By the law of large numbers, as <span class="math inline">\(\sum\limits_{i=1}^{t-1}\mathbb{1}_{A_{i}=a} \rightarrow \infty\)</span>, <span class="math inline">\(Q_t(a) \rightarrow q_*(a)\)</span>.</p>
</div>
<div class="fragment">
<p>This is known as the <em>sample-average</em> method for estimating action values because each estimate is an average of the sample of relevant rewards.</p>
</div>
</section>
<section id="section" class="slide level2">
<h2></h2>
<div class="fragment">
<p>The simplest action selection rule is to select one of the actions with the highest estimated value, that is, one of the greedy actions. If there is more than one greedy action, then a selection is made among them in some arbitrary way, perhaps randomly. We write this greedy action selection method as</p>
</div>
<div class="fragment">
<p><span class="math display">\[ A_t  \doteq \text{argmax}_a Q_t(a) \]</span></p>
</div>
<div class="fragment">
<p>Greedy action selection always exploits current knowledge to maximize immediate reward; it spends no time at all sampling apparently inferior actions to see if they might really be better.</p>
</div>
</section>
<section id="epsilon-greedy-methods" class="slide level2">
<h2><span class="math inline">\(\epsilon\)</span>-Greedy Methods</h2>
<div class="fragment">
<p>A simple alternative to allow for some exploration is to behave greedily most of the time, but every once in a while, say with small probability <span class="math inline">\(\epsilon\)</span>, instead select randomly from among all the <span class="math inline">\(k = \left| \mathcal{A} \right|\)</span> actions with equal probability, independently of the action-value estimates.</p>
</div>
<div class="fragment">
<p>We call methods using this near-greedy action selection rule <span class="math inline">\(\epsilon\)</span>-greedy methods.</p>
</div>
<div class="fragment">
<p>The <span class="math inline">\(\epsilon\)</span>-greedy action selection method is defined as follows:</p>
<ul>
<li class="fragment"><p>With probability <span class="math inline">\(1-\epsilon\)</span>, select <span class="math inline">\(A_t = \text{argmax}_a Q_t(a)\)</span> (<em>greedy</em> action)</p></li>
<li class="fragment"><p>With probability <span class="math inline">\(\epsilon\)</span>, select <span class="math inline">\(A_t\)</span> randomly from <span class="math inline">\(\mathcal{A}\)</span></p></li>
</ul>
</div>
<div class="fragment">
<p><span class="math inline">\(\implies P(A_t = a) = 1 - \epsilon + \frac{\epsilon}{k}\ \)</span></p>
</div>
</section>
<section id="softmax-action-selection" class="slide level2">
<h2>Softmax Action Selection</h2>
<div class="fragment">
<p>The softmax action selection rule is a “soft” version of the greedy action selection rule.</p>
</div>
<div class="fragment">
<p>In <span class="math inline">\(\epsilon\)</span>-greedy methods, the greedy action gets the most probability mass, and all the other actions have an equal probability of being selected in the exploration phase.</p>
</div>
<div class="fragment">
<p>If some actions are worse than others, should try to reduce the probability of selecting them during exploration.</p>
</div>
<div class="fragment">
<p>The softmax action selection method can be denoted as</p>
</div>
<div class="fragment">
<p><span class="math display">\[ P(A_t = a) = \frac{e^{Q_t(a)/\tau}}{\sum_{b=1}^{k}e^{Q_t(b)/\tau}} \]</span></p>
</div>
<div class="fragment">
<p>where <span class="math inline">\(\tau\)</span> is the <em>temperature</em> parameter that controls the level of exploration. It can be “cooled” over time to reduce exploration.</p>
</div>
</section>
<section id="incremental-implementation" class="slide level2">
<h2>Incremental Implementation</h2>
<div class="fragment">
<p>The action-value methods discussed so far all estimate action values as sample averages of observed rewards.</p>
</div>
<div class="fragment">
<p>To simplify notation, we focus on a single action. Let <span class="math inline">\(R_i\)</span> now denote the reward received after the <span class="math inline">\(i\)</span>th selection <em>of this action</em>, and let <span class="math inline">\(Q_n\)</span> denote the estimate of its action value after it has been selected <span class="math inline">\(n-1\)</span> times, which we can now write simply as</p>
</div>
<div class="fragment">
<p><span class="math display">\[ Q_{n} \doteq \frac{R_{1} + R_{2} + \ldots + R_{n-1}}{n-1} \]</span></p>
</div>
<div class="fragment">
<p>The obvious implementation would be to maintain a record of all the rewards and then perform this computation whenever the estimated value was needed. However, if all the rewards are stored, as more rewards are seen, the memory and computational requirements would keep increasing.</p>
</div>
</section>
<section id="section-1" class="slide level2 smaller">
<h2></h2>
<div class="fragment">
<p>These averages can be computed in a computationally efficient manner, in particular, with constant memory and constant per-time-step computation. Given <span class="math inline">\(Q_{n}\)</span> and the <span class="math inline">\(n\)</span>th reward, <span class="math inline">\(R_{n}\)</span>, the new average of all <span class="math inline">\(n\)</span> rewards can be computed by</p>
</div>
<div class="fragment">
<p><span class="math inline">\(\begin{aligned}
Q_{n+1} &amp; =\frac{1}{n} \ \sum\limits_{i=1}^{n} R_{i}\\
&amp; =\frac{1}{n} \ \left( R_{n} +\sum\limits_{i=1}^{n-1} R_{i}\right)\\
&amp; =\frac{1}{n} \ \left( R_{n} +( n-1)\frac{1}{n-1} \ \sum\limits _{i=1}^{n-1} R_{i}\right)\\
&amp; =\frac{1}{n} \ ( R_{n} +( n-1) Q_{n})\\
&amp; =\frac{1}{n} \ ( R_{n} +nQ_{n} -Q_{n})\\
&amp; =Q_{n} +\frac{1}{n}[ R_{n} -Q_{n}]
\end{aligned}\)</span></p>
</div>
<div class="fragment">
<p>which holds even for <span class="math inline">\(n=1\)</span>, obtaining <span class="math inline">\(Q_{2} = R_{1}\)</span> for arbitrary <span class="math inline">\(Q_{1}\)</span>.</p>
</div>
</section>
<section id="section-2" class="slide level2">
<h2></h2>
<div class="fragment">
<p>This update rule has the form of a stochastic averaging equation. The general form of such an equation is</p>
</div>
<div class="fragment">
<p><span class="math display">\[ NewEstimate \leftarrow OldEstimate + StepSize \left[ Target - OldEstimate \right] \]</span></p>
</div>
<div class="fragment">
<p>The expression <span class="math inline">\([Target - OldEstimate]\)</span> is an error in the estimate. It is reduced by taking a step toward the “Target”.</p>
</div>
<div class="fragment">
<p>The step-size parameter <span class="math inline">\((StepSize)\)</span> used in the incremental implementation changes from time step to time step.</p>
</div>
<div class="fragment">
<p>In processing the <span class="math inline">\(n\)</span>th reward for an action, the step-size parameter is <span class="math inline">\(\frac{1}{n} \\\)</span>. It is often denoted by <span class="math inline">\(\alpha\)</span> or, more generally, by <span class="math inline">\(\alpha_{t}(a)\)</span>.</p>
</div>
</section>
<section id="nonstationary-problems" class="slide level2">
<h2>Nonstationary Problems</h2>
<div class="fragment">
<p>The methods discussed so far are appropriate for stationary bandit problems, where the reward probabilities do not change over time.</p>
</div>
<div class="fragment">
<p>In nonstationary cases, it makes sense to give more weight to recent rewards than to long-past rewards.</p>
</div>
<div class="fragment">
<p>One way to do this is to use a constant step-size parameter. The incremental update rule for updating an average <span class="math inline">\(Q_{n}\)</span> of the <span class="math inline">\(n-1\)</span> past rewards is modified to be</p>
</div>
<div class="fragment">
<p><span class="math display">\[ Q_{n+1} = Q_{n} + \alpha \left[ R_{n} - Q_{n} \right] \]</span></p>
</div>
<div class="fragment">
<p>where the step-size parameter <span class="math inline">\(\alpha \in (0, 1]\)</span> is constant.</p>
</div>
</section>
<section id="section-3" class="slide level2">
<h2></h2>
<div class="fragment">
<p>This results in <span class="math inline">\(Q_{n+1}\)</span> being a weighted average of past rewards and the initial estimate <span class="math inline">\(Q_{1}\)</span></p>
</div>
<div class="fragment">
<p><span class="math inline">\(\begin{array}{ r c l }
Q_{n+1} &amp; = &amp; Q_{n} +\alpha [R_{n} -Q_{n} ]\\
&amp; = &amp; \alpha R_{n} +(1-\alpha )Q_{n}\\
&amp; = &amp; \alpha R_{n} +(1-\alpha )[\alpha R_{n-1} +(1-\alpha )Q_{n-1} ]\\
&amp; = &amp; \alpha R_{n} +(1-\alpha )\alpha R_{n-1} +(1-\alpha )^{2} Q_{n-1}\\
&amp; = &amp; \alpha R_{n} +(1-\alpha )\alpha R_{n-1} +(1-\alpha )^{2} \alpha R_{n-2} +\\
&amp;  &amp; \dotsc +(1-\alpha )^{n-1} \alpha R_{1} +(1-\alpha )^{n} Q_{1}\\
&amp; = &amp; \ (1-\alpha )^{n} Q_{1} +\sum\limits _{i=1}^{n} \alpha (1-\alpha )^{n-i} R_{i}
\end{array}\)</span></p>
</div>
</section>
<section id="section-4" class="slide level2">
<h2></h2>
<div class="fragment">
<p>We call this a weighted average because the sum of the weights is <span class="math inline">\((1-\alpha )^{n} +\sum\limits _{i=1}^{n} \alpha (1-\alpha )^{n-i} = 1\)</span>.</p>
</div>
<div class="fragment">
<p>Note that the weight <span class="math inline">\(\alpha (1-\alpha )^{n-i}\)</span>, given to the reward <span class="math inline">\(R_{i}\)</span>, depends on how many rewards ago, <span class="math inline">\(n-i\)</span>, it was observed.</p>
</div>
<div class="fragment">
<p>The quantity <span class="math inline">\(1-\alpha\)</span> is less than <span class="math inline">\(1\)</span>, so the weight given to <span class="math inline">\(R_i\)</span> decreases as the number of intervening rewards increases.</p>
</div>
<div class="fragment">
<p>In fact, the weight decays exponentially according to the exponent on <span class="math inline">\(1-\alpha\)</span>. If <span class="math inline">\(1-\alpha = 0\)</span>, then all the weight goes on the very last reward, <span class="math inline">\(R_n\)</span>, because of the convention that <span class="math inline">\(0^0 = 1\)</span>.</p>
</div>
<div class="fragment">
<p>Accordingly, this is sometimes called an <em>exponential recency-weighted average</em>.</p>
</div>
</section>
<section id="upper-confidence-bound-ucb-action-selection" class="slide level2">
<h2>Upper-Confidence-Bound (UCB) Action Selection</h2>
<ul>
<li class="fragment">Simply put, a <span class="math inline">\(K\)</span>-armed bandit problem is defined by random variables <span class="math inline">\(X_{i,n}\)</span> for <span class="math inline">\(1 \leq i \leq K\)</span> and <span class="math inline">\(n \geq 1\)</span>, where <span class="math inline">\(i\)</span> is the index of a gambling machine (i.e., the “arm” of the bandit).</li>
<li class="fragment">Successive plays of arm <span class="math inline">\(i\)</span> yield rewards <span class="math inline">\(X_{i,1}, X_{i,2}, \ldots\)</span> that are independent and identically distributed (i.i.d.) random variables to an unknown law with unknown expectation <span class="math inline">\(\mu_i\)</span>.</li>
<li class="fragment">Independence also holds for rewards across different arms; i.e., <span class="math inline">\(X_{i,s}\)</span> and <span class="math inline">\(X_{j,t}\)</span> are independent (and usually not identically distributed) for each <span class="math inline">\(1 \leq i &lt; j \leq K\)</span> and each <span class="math inline">\(s, t \geq 1\)</span>.</li>
</ul>
</section>
<section id="regret" class="slide level2">
<h2>Regret</h2>
<ul>
<li class="fragment">A <em>policy</em>, or <em>allocation strategy</em>, <span class="math inline">\(A\)</span> is an algorithm that chooses the next arm to play based on the sequence of past plays and obtained rewards.</li>
<li class="fragment">Let <span class="math inline">\(T_{i}(n)\)</span> be the random variable which represents the number of times arm <span class="math inline">\(i\)</span> has been played by <span class="math inline">\(A\)</span> during the first <span class="math inline">\(n\)</span> plays.</li>
<li class="fragment">Then the <em>regret</em> of <span class="math inline">\(A\)</span> after <span class="math inline">\(n\)</span> plays is defined by <span class="math display">\[\mu^* n - \mu_{j} \sum\limits_{i=1}^{K} \mathbb{E}[T_{j}(n)]\]</span> where <span class="math inline">\(\mu^* = \underset{1\leq i \leq K}{\max} \mu_i\)</span> and <span class="math inline">\(\mathbb{E}[\cdot]\)</span> denotes the expectation.</li>
<li class="fragment">Thus the regret is the expected loss due to the fact that the policy does not always play the best arm.</li>
</ul>
</section>
<section id="ucb1-algorithm" class="slide level2">
<h2>UCB1 Algorithm</h2>
<div class="fragment">
<div class="line-block"><strong>Deterministic policy</strong>: UCB1.<br>
<strong>Initialization</strong>: Play each arm once.<br>
<strong>Loop</strong>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Play arm <span class="math inline">\(j\)</span> that maximizes <span class="math inline">\(\bar{x}_j + \sqrt{\frac{2\ln n}{n_j}}\)</span>,</div>
<p>where <span class="math inline">\(\bar{x}_j\)</span> is the average reward obtained from arm <span class="math inline">\(j\)</span>, <span class="math inline">\(n_j\)</span> is the number of times arm <span class="math inline">\(j\)</span> has been played so far, and <span class="math inline">\(n\)</span> is the overall number of plays so far.</p>
</div>
<div class="fragment">
<p><strong>Theorem</strong>: For all <span class="math inline">\(K &gt; 1\)</span>, if policy UCB1 is run on <span class="math inline">\(K\)</span> arms having arbitrary reward distributions <span class="math inline">\(P_1,\dots,P_{K}\)</span> with support in <span class="math inline">\([0, 1]\)</span>, then its expected regret after any number of plays <span class="math inline">\(n\)</span> is at most <span class="math display">\[ \left[8 \sum\limits_{i:\mu_i &lt; \mu^*} \left( \frac{\ln n}{\Delta_{i}} \right) \right] + \left(1+ \frac{\pi^2}{3}  \right) \left(\sum\limits_{j=1}^{K} \Delta_{j} \right)\]</span> where <span class="math inline">\(\mu_1,\dots,\mu_K\)</span> are the expected values of <span class="math inline">\(P_1,\dots,P_{K}\)</span>.</p>
</div>
</section>
<section id="section-5" class="slide level2">
<h2></h2>
<div class="fragment">
<ul>
<li class="fragment">Matching the notation of the theorem from the original paper to our notation, we get:
<ul>
<li class="fragment"><span class="math inline">\(\bar{x}_j = Q(j)\)</span></li>
<li class="fragment"><span class="math inline">\(\mu_i = \mathbb{E}[X_{i,n}] = q_*(i)\)</span></li>
<li class="fragment"><span class="math inline">\(\mu^* = q_*(a^*)\)</span></li>
<li class="fragment"><span class="math inline">\(\Delta_{i} = \mu^* - \mu_i = q_*(a^*) - q_*(i)\)</span></li>
<li class="fragment"><span class="math inline">\(\text{Regret}_{n}=\sum\limits_{i} \mathbb{E}[T_{i}(n)] \Delta_{i}\)</span></li>
</ul></li>
</ul>
</div>
<ul>
<li class="fragment">To prove the theorem, we need to show that, for any suboptimal arm <span class="math inline">\(j\)</span>,</li>
</ul>
<div class="fragment">
<p><span class="math display">\[\mathbb{E}[T_{j}(n)] \leq \frac{8 \ln n}{\Delta_{j}^2}+c\]</span></p>
</div>
<ul>
<li class="fragment">We also define the r.v.’s <span class="math inline">\(I_1, I_2, \ldots\)</span> such that $I_t denotes the arm played at time <span class="math inline">\(t\)</span>. Also, <span class="math inline">\(\sum_{i=1}^{K} T_{i}(n) = n\)</span>. For each <span class="math inline">\(1\leq i \leq K\)</span>, and <span class="math inline">\(n \geq 1\)</span>, define</li>
</ul>
<div class="fragment">
<p><span class="math display">\[\bar{X}_{i,n} = \frac{1}{n} \sum\limits_{t=1}^{n} X_{i,t} \]</span></p>
</div>
</section>
<section id="sec-Chernoff-Hoeffding-Bound" class="slide level2">
<h2>Chernoff-Hoeffding Bound</h2>
<div class="fragment">
<p>Let <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> be random variables with common range <span class="math inline">\([0, 1]\)</span> and such that <span class="math inline">\(\mathbb{E}[X_t|X_1, X_2, \ldots, X_{t-1}] = \mu\)</span>. Let <span class="math inline">\(S_n = \frac{X_1 + X_2 + \ldots + X_n}{n}\)</span>. Then, for all <span class="math inline">\(\epsilon &gt; 0\)</span>,</p>
</div>
<div class="fragment">
<p><span class="math display">\[ P(S_n \geq \mu + \epsilon) \leq e^{-2\epsilon^{2}n} \text{ and } P(S_n \leq \mu - \epsilon) \leq e^{-2\epsilon^{2}n} \]</span></p>
</div>
</section>
<section id="proof-of-ucb1-theorem" class="slide level2">
<h2>Proof of UCB1 Theorem</h2>
<ul>
<li class="fragment">Let <span class="math inline">\(c_{t,s} = \sqrt{\frac{2\ln t}{s}}\)</span>.</li>
<li class="fragment">For any arm <span class="math inline">\(i\)</span>, we upper bound <span class="math inline">\(T_{i}(n)\)</span> on any sequence of plays.</li>
<li class="fragment">More precisely, for each <span class="math inline">\(t \geq 1\)</span> we bound the indicator function of <span class="math inline">\(I_t=i\)</span> as follows.</li>
</ul>
<div class="fragment">
<p>Let <span class="math inline">\(\mathscr{l}\)</span> be an arbitrary positive integer. Note: <span class="math inline">\(\displaystyle \{I_{t} =i\} =1\)</span> if arm <span class="math inline">\(i\)</span> is played at time <span class="math inline">\(t\)</span> and <span class="math inline">\(0\)</span> otherwise.</p>
</div>
</section>
<section id="section-6" class="slide level2">
<h2></h2>
<div class="fragment">
<span class="math display">\[\begin{aligned}
T_i(n) &amp;= 1 + \sum\limits_{t=K+1}^{n} \{I_{t} =i\} \\
&amp;\leq \mathscr{l} + \sum\limits_{t=K+1}^{n} \{I_{t} =i, T_{i}(t-1) \geq \mathscr{l} \} \\
&amp;\leq \mathscr{l} + \sum\limits_{t=K+1}^{n} \{\bar{X}_{T^{*}(t-1)}^{*} + c_{t-1,T^{*}(t-1)} \leq \bar{X}_{i,T_{i}(t-1)} + c_{t-1,T_{i}(t-1)}, T_{i}(t-1) \geq \mathscr{l} \}  \\
&amp;\leq \mathscr{l} + \sum\limits_{t=K+1}^{n} \{ \underset{0&lt;s&lt;t}{\min} \bar{X}_{s}^{*} + c_{t-1,s} \leq \underset{\mathscr{l}\ \leq s_i &lt; t}{\max} \bar{X}_{i,s_i} + c_{t-1,s_i} \} \\

&amp;\leq \mathscr{l} + \sum\limits_{t=1}^{\infty} \sum\limits_{s=1}^{t-1} \sum\limits_{s_{i}=\mathscr{l}}^{t-1} \{ \bar{X}_{s}^{*} + c_{t,s} \leq \bar{X}_{i, s_{i}} + c_{t,s_i} \}
\end{aligned}\]</span>
</div>
<ul>
<li class="fragment">Now observe that <span class="math inline">\(\bar{X}_{s}^{*} + c_{t,s} \leq \bar{X}_{i, s_{i}} + c_{t,s_i}\)</span> implies that at least one of the following must hold
<ul>
<li class="fragment"><span class="math inline">\(\overline{X}_{s}^{*} +c_{t,s} \leqslant \ \mu ^{*} -c_{t,s}\)</span></li>
<li class="fragment"><span class="math inline">\(\overline{X}_{i,s_{i}} \geqslant \mu _{i} +c_{t,s_{i}}\)</span></li>
<li class="fragment"><span class="math inline">\(\mu ^{*} &lt; \mu _{i} +2c_{t,s_{i}}\)</span></li>
</ul></li>
</ul>
</section>
<section id="section-7" class="slide level2">
<h2></h2>
<ul>
<li class="fragment"><p>We bound the probability of the first two events using the Chernoff-Hoeffding bound as shown in <a href="#/sec-Chernoff-Hoeffding-Bound" class="quarto-xref">Section&nbsp;19</a>.</p></li>
<li class="fragment"><p><span class="math inline">\(P(\overline{X}_{s}^{*} +c_{t,s} \leqslant \ \mu ^{*} -c_{t,s}) \leq e^{-2c_{t,s}^{2}s} = e^{-4\ln t} = t^{-4}\)</span></p></li>
<li class="fragment"><p><span class="math inline">\(P(\overline{X}_{i,s_{i}} \geqslant \mu _{i} +c_{t,s_{i}}) \leq e^{-2c_{t,s_{i}}^{2}s_{i}} = e^{-4\ln t} = t^{-4}\)</span></p></li>
<li class="fragment"><p>For <span class="math inline">\(\mathscr{l} =\left\lceil \frac{8\ \ln n}{\Delta _{i}^{2}}\right\rceil\)</span>, the third event is impossible. In fact</p></li>
</ul>
<div class="fragment">
<p><span class="math inline">\(\mu ^{*} - \mu _{i} - 2c_{t,s_{i}} = \mu ^{*} - \mu _{i} - 2\sqrt{\frac{2\ln t}{s_{i}}} \geq \mu ^{*} - \mu _{i} - \Delta _{i} = \Delta _{i} - \Delta _{i} = 0\)</span></p>
</div>
<div class="fragment">
<p>for <span class="math inline">\(s_i \geq \frac{8\ln n}{\Delta_i^2}\)</span>. So we get</p>
</div>
<div class="fragment">
<span class="math display">\[\begin{array}{ c c l }
\mathbb{E}[ T_{i}( n)] &amp; \leqslant  &amp;  \begin{array}{l}
\left\lceil \frac{8\ \ln n}{\Delta _{i}^{2}}\right\rceil +\sum\limits _{t=1}^{\infty }\sum\limits _{s=1}^{t-1}\sum\limits _{s_{i} =\left\lceil \frac{8\ \ln n}{\Delta _{i}^{2}}\right\rceil }^{t-1}\\
\times \ \left( P\left\{\overline{X}_{s}^{*} \leqslant \mu ^{*} -c_{t,s}\right\} +P\{\overline{X}_{i,s_{i}} \geqslant \mu _{i} +c_{t,s_{i}}\}\right)
\end{array}\\
&amp; \leqslant  &amp; \left\lceil \frac{8\ \ln n}{\Delta _{i}^{2}}\right\rceil +\sum\limits _{t=1}^{\infty }\sum\limits _{s=1}^{t}\sum\limits _{s_{i} =1}^{t} 2t^{-4}\\
&amp; \leqslant  &amp; \frac{8\ \ln n}{\Delta _{i}^{2}} +1+\frac{\pi ^{2}}{3}
\end{array}\]</span>
</div>
<div class="fragment">
<p>which concludes the proof.</p>
</div>

<div class="quarto-auto-generated-content">
<div class="footer footer-default">
<p>See full page on multi-armed bandits <a href="../../pages/RL/Multi-armed-bandits.html">here</a></p>
</div>
</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': true,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        // For code content inside modals, clipBoardJS needs to be initialized with a container option
        // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp("https:\/\/arachnidly\.github\.io\/learning\/");
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>