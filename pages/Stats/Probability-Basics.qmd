---
title: "Basic Concepts of Probability"
format:
    html:
        toc: true
        toc-depth: 2
        html-math-method: katex
license:
  text: "This work is dedicated to the Public Domain"
---

## Experiment and Outcome

- Experiment
  : Process or phenomenon that we wish to study statistically
- Outcome
  : Result of the experiment (in as much detail as necessary)

## Sample Space

**Definition**: A sample space $S$ is a set that contains all possible outcomes of an experiment.

- Sample space is a set, typically denoted $S$
- Examples:
  - Flipping a coin: $S = \{\text{Heads}, \text{Tails}\}$
  - Rolling a die: $S = \{1, 2, 3, 4, 5, 6\}$

## Event

**Definition**: An event is a subset of the sample space.

- Examples:
  - Toss a coin: $S = \{\text{Heads}, \text{Tails}\}$
    - Events: $\emptyset$, $\{\text{Heads}\}$, $\{\text{Tails}\}$, $\{\text{Heads}, \text{Tails}\}$
    - $4$ events
  - Throw a die: $S = \{1, 2, 3, 4, 5, 6\}$
    - Events: $\emptyset$, $\{1\}$, $\{2\}$, $\{3\}$, $\{4\}$, $\{5\}$, $\{6\}$, $\{1, 2\}$, $\{1, 3\}$, $\ldots$, $\{1, 2, 3, 4, 5, 6\}$
    - $64$ events

- An event is said to have "occurred" if the actual outcome of the experiment belongs to the event.
- Events are sets
  - All set theory notions apply to events
- One event can be contained in another, i.e. $A \subseteq B$
  - Throw a die: $A = \{2, 6\}$, $B = \text{Even number}$
  - If $A$ occured, $B$ has also occured
  - If $B$ occured, $A$ may or may not have occured
- Complement of an event $A$, denoted $A^c = \{\text{outcomes in } S \text{ not in } A\} = (S \setminus A)$
  - Throw a die: $A = \{2, 4, 6\}$ (even), $A^c = \{1, 3, 5\}$ (odd)
  - If $A$ occured, $A^c$ did not occur
  - If $A^c$ occured, $A$ did not occur

### Combining events to create new events

- Union of events $A$ and $B$, denoted $A \cup B = \{x: x \in A \text{ or } x \in B\}$
  - Throw a die: $A = \{1, 2, 3\}$, $B = \{3, 4, 5\}$, $A \cup B = \{1, 2, 3, 4, 5\}$
- Intersection of events $A$ and $B$, denoted $A \cap B = \{x: x \in A \text{ and } x \in B\}$
  - Throw a die: $A = \{1, 2, 3\}$, $B = \{3, 4, 5\}$, $A \cap B = \{3\}$

### Disjoint events

- Two events with an empty intersection are said to be disjoint or mutually exclusive events
  - Throw a die: even number, odd number are disjoint
- Suppose $A$ and $B$ are disjoint events
  - If $A$ occured, $B$ did not occur
  - If $B$ occured, $A$ did not occur
- Event and its complement
  - $A$ and $A^c$ are disjoint, i.e. $A \cap A^c = \emptyset$
  - Together, they cover the entire sample space, i.e. $A \cup A^c = S$
    - Either $A$ or $A^c$ must occur
  - $A$ and $A^c$ are an example of a partition of the sample space
    - Partition: Collection of disjoint sets that together cover the entire sample space
- Multiple events: $E_1, E_2, \ldots, E_n$ are disjoint if, for any $i \neq j$, $E_i \cap E_j = \emptyset$

## Probability

**Definition**: "Probability" is a function $P$ that assigns to each event a real number between $0$ and $1$. The entire probability space (sample space, events and probability function) should satisfy the following two axioms:

1. $P(S) = 1$ (probability of the entire sample space equals $1$)

2. If $E_1, E_2, E_3, \ldots$ are disjoint events (how many events? Could be infinitely many), $$P(E_1 \cup E_2 \cup E_3 \cup \ldots) = P(E_1) + P(E_2) + P(E_3) + \ldots$$

## Basic Properties of Probability

::: {#thm-basic-properties-probability}
## Basic Properties of Probability
Let $P$ be a probability on a sample space $S$. Then,

1. $P(\emptyset) = 0$

2. Let $E^c$ be the complement of Event $E$. Then, $$\newline P(E^c) = 1 - P(E)$$

3. If Event $E$ is a subset of Event $F$, i.e. $E \subseteq F$, then $\newline$ $$P(F) = P(E) + P(F \setminus E)\text{,}$$ which implies that $P(E) \leq P(F)$
   
    3.a. If $E$ and $F$ are events, then $$P(E) = P(E \cap F) + P(E \setminus F) \text{,}$$ $$P(F) = P(E \cap F) + P(F \setminus E)$$

4. If $E$ and $F$ are events, then $$P(E \cup F) = P(E) + P(F) - P(E \cap F)$$
:::

::: {.proof}


(@) - $\emptyset^c = S$ and $\emptyset$, $S$ are disjoint and $\emptyset \cup S = S$.
    - By Axiom 2, $P(\emptyset \cup S) = P(\emptyset) + P(S)$ or $P(S) = P(\emptyset) + P(S)$ or $P(\emptyset) = 0$.

(@) - $E$ and $E^c$ are disjoint and $E \cup E^c = S$.
    - By Axiom 2, $P(E \cup E^c) = P(E) + P(E^c)$ or $P(S) = P(E) + P(E^c)$
    - By Axiom 1, $1 = P(S) = P(E) + P(E^c)$
    - So, $P(E^c) = 1 - P(E)$

(@) - $F \setminus E = F \cap E^c$ (outside of $E$ and inside of $F$)
    - $E$ and $F \setminus E$ are disjoint and $E \cup (F \setminus E) = F$
    - By Axiom 2, $P(E \cup (F \setminus E)) = P(E) + P(F \setminus E)$ or $P(F) = P(E) + P(F \setminus E)$
    
    3.a.
      - $E \cap F$ is a subset of $E$
      - By subset property, $P(E) = P(E \cap F) + P(E \setminus (E \cap F))$
      - Now, $E \setminus (E \cap F) = E \setminus F$
      - So, $P(E) = P(E \cap F) + P(E \setminus F)$

(@) - $E \cup F = (E \setminus F) \cup (E \cap F) \cup (F \setminus E)$ and the $3$ events on RHS are disjoint
    - $P(E \cup F) = P(E \setminus F) + P(E \cap F) + P(F \setminus E)$
    - Use $P(E \setminus F) = P(E) - P(E \cap F)$ and $P(F \setminus E) = P(F) - P(E \cap F)$

:::

## Conditional Probability Space

**Definition**: Consider a probability space: Sample space $S$, collection of events, and a probability function $P$. Let $B$ be an event with $P(B) > 0$.

$\newline$Sample space: B
$\newline$Events: $A \cap B$ for every event $A$ in the original space

$$\text{Probability function: } \frac{P(A \cap B)}{P(B)}$$

$\newline$(denoted $P(A|B)$ and called *conditional probability* of $A$ given $B$)

For any event $A$ in the original space, $$P(A \cap B) = P(B) \cdot P(A|B)$$

## Law of Total Probability

::: {#thm-law-total-probability}

## Law of Total Probability

$$B_1, B_2, \ldots \text{ : Partition of } S$$
$$ P(A) = P(A \cap B_1) + P(A \cap B_2) + \ldots = P(A|B_1) P(B_1) + P(A|B_2) P(B_2) + \ldots$$

:::

::: {.proof}

- $A$: disjoint union of $A \cap B_1, A \cap B_2, \ldots$
- By Axiom 2, $P(A) = P(A \cap B_1) + P(A \cap B_2) + \ldots$
- Using conditional probability on each term above, we get the result

:::

## Bayes' Theorem

::: {#thm-bayes-theorem}

## Bayes' Theorem

$$ A,B \text{: Events with } P(A) > 0 \text{, } P(B) > 0$$
$$P(A \cap B) = P(B)P(A|B) = P(A)P(B|A)$$
$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$

:::

::: {.proof}

- Bayes' Theorem may be derived from the definition of conditional probability: $$P(A|B) = \frac{P(A \cap B)}{P(B)} \text{, if } P(B) \neq 0,$$

- where $P(A \cap B)$ is the probability of both $A$ and $B$ being true. Similarly, $$P(B|A) = \frac{P(A \cap B)}{P(A)} \text{, if } P(A) \neq 0.$$

- Solving for $P(A \cap B)$ and substituting into the above expression for $P(A|B)$ yields Bayes' Theorem: $$P(A|B) = \frac{P(B|A)P(A)}{P(B)} \text{, if } P(B) \neq 0.$$

:::

## Independence of Events

**Definition**: Two events $A$ and $B$ are independent if $$P(A \cap B) = P(A)P(B)$$

- If $P(B) > 0$, $P(A|B) = P(A)$
- Disjoint events are never independent
- For events to be independent, they should have a non-empty intersection

## Mutual independence of multiple events

**Definition**: Events $A_1, A_2, \ldots, A_n$ are mutually independent if, for all $i_1, i_2, \ldots, i_k$, $$P(A_{i_1} \cap A_{i_2} \cap \ldots \cap A_{i_k}) = P(A_{i_1})P(A_{i_2}) \ldots P(A_{i_k})$$

- $A$ and $B$ are independent $\implies$ $A$ and $B^c$ are independent
  - $P(A \cap B^c) = P(A \setminus B) = P(A) - P(A \cap B) = P(A)(1 - P(B)) = P(A)P(B^c)$
  - Intuitive: $B$ does not affect $A \implies$ $B^c$ does not affect $A$
  - Two events are independent $\implies$ Complement of one event is independent of the other
- Using the above twice: $A$ and $B$ are independent $\implies$ $A^c$ and $B^c$ are independent
- Extension: $n$ events are mutually independent $\implies$ any subset with or without complementing are independent as well