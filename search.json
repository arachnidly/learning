[
  {
    "objectID": "pages/RL/Multi-armed-bandits.html#a-k-armed-bandit-problem",
    "href": "pages/RL/Multi-armed-bandits.html#a-k-armed-bandit-problem",
    "title": "Multi-armed Bandits",
    "section": "A \\(k\\)-armed Bandit Problem",
    "text": "A \\(k\\)-armed Bandit Problem\n\nYou are faced repeatedly with a choice among \\(k\\) different actions.\n\n\nAfter each choice, you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected.\n\n\nYour objective is to maximize the expected total reward over some time period, for example, over \\(1000\\) action selections, or time steps.",
    "crumbs": [
      "Reinforcement Learning",
      "Multi-armed Bandits"
    ]
  },
  {
    "objectID": "pages/RL/Multi-armed-bandits.html#value-of-an-action",
    "href": "pages/RL/Multi-armed-bandits.html#value-of-an-action",
    "title": "Multi-armed Bandits",
    "section": "Value of an Action",
    "text": "Value of an Action\n\nThe set of all actions is denoted by \\(\\mathcal{A}\\), \\(\\left| \\mathcal{A} \\right| = k\\).\n\n\nEach of the \\(k\\) actions has an expected reward given the action is selected - the value of the action.\n\n\nThe action selected on time step \\(t\\) is \\(A_t\\).\n\n\nThe corresponding reward is \\(R_t\\).\n\n\nThe value of an arbitrary action \\(a\\), denoted \\(q_*(a)\\), is the expected reward given that \\(a\\) is selected:\n\n\n\\[ q_*(a) = \\mathbb{E}[R_t | A_t = a] \\]",
    "crumbs": [
      "Reinforcement Learning",
      "Multi-armed Bandits"
    ]
  },
  {
    "objectID": "pages/RL/Multi-armed-bandits.html#the-problem",
    "href": "pages/RL/Multi-armed-bandits.html#the-problem",
    "title": "Multi-armed Bandits",
    "section": "The Problem",
    "text": "The Problem\n\nThe value of an action is unknown and must be estimated.\n\n\nWe denote the estimated value of action \\(a\\) at time step \\(t\\) as \\(Q_t(a)\\).\n\n\nWe would like \\(Q_t(a)\\) to be close to \\(q_*(a)\\).\n\n\nShould we select the action with the highest estimated value or should we explore other actions? This is the exploration-exploitation dilemma/trade-off.",
    "crumbs": [
      "Reinforcement Learning",
      "Multi-armed Bandits"
    ]
  },
  {
    "objectID": "pages/RL/Multi-armed-bandits.html#possible-solutions",
    "href": "pages/RL/Multi-armed-bandits.html#possible-solutions",
    "title": "Multi-armed Bandits",
    "section": "Possible solutions",
    "text": "Possible solutions\n\nAsymptotic correctness\n\n\n\\[Q_t(a) \\rightarrow q_*(a) \\text{ as } t \\rightarrow \\infty\\]\n\n\nRegret optimality\n\n\n\\[\\lim_{t \\rightarrow \\infty} \\frac{1}{t} \\sum_{i=1}^{t} (q_*(a^*) - q_*(A_t)) = 0\\]\n\n\nPAC (Probably Approximately Correct) optimality\n\n\n\\[P(q_*(A_t) \\geq q_*(a^*) - \\epsilon) \\geq (1-\\delta)\\]",
    "crumbs": [
      "Reinforcement Learning",
      "Multi-armed Bandits"
    ]
  },
  {
    "objectID": "pages/RL/Multi-armed-bandits.html#action-value-methods",
    "href": "pages/RL/Multi-armed-bandits.html#action-value-methods",
    "title": "Multi-armed Bandits",
    "section": "Action-value Methods",
    "text": "Action-value Methods\n\nThe true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received:\n\n\n\\[ Q_t(a) = \\frac{\\text{sum of rewards when } a \\text{ taken prior to } t}{\\text{number of times } a \\text{ taken prior to } t} = \\frac{\\sum\\limits_{i=1}^{t-1}R_{i}\\cdot\\mathbb{1}_{A_{i}=a}}{\\sum\\limits_{i=1}^{t-1}\\mathbb{1}_{A_{i}=a}} \\]\n\n\nwhere \\(\\mathbb{1}_{\\text{predicate}}\\) denotes the random variable that is \\(1\\) if the predicate is true and \\(0\\) otherwise.\n\n\nIf the denominator is \\(0\\), we define \\(Q_t(a)\\) to be some default value, such as \\(0\\).\n\n\nBy the law of large numbers, as \\(\\sum\\limits_{i=1}^{t-1}\\mathbb{1}_{A_{i}=a} \\rightarrow \\infty\\), \\(Q_t(a) \\rightarrow q_*(a)\\).\n\n\nThis is known as the sample-average method for estimating action values because each estimate is an average of the sample of relevant rewards.",
    "crumbs": [
      "Reinforcement Learning",
      "Multi-armed Bandits"
    ]
  },
  {
    "objectID": "pages/RL/Multi-armed-bandits.html#section",
    "href": "pages/RL/Multi-armed-bandits.html#section",
    "title": "Multi-armed Bandits",
    "section": "",
    "text": "The simplest action selection rule is to select one of the actions with the highest estimated value, that is, one of the greedy actions. If there is more than one greedy action, then a selection is made among them in some arbitrary way, perhaps randomly. We write this greedy action selection method as\n\n\n\\[ A_t  \\doteq \\text{argmax}_a Q_t(a) \\]\n\n\nGreedy action selection always exploits current knowledge to maximize immediate reward; it spends no time at all sampling apparently inferior actions to see if they might really be better.",
    "crumbs": [
      "Reinforcement Learning",
      "Multi-armed Bandits"
    ]
  },
  {
    "objectID": "pages/RL/Multi-armed-bandits.html#epsilon-greedy-methods",
    "href": "pages/RL/Multi-armed-bandits.html#epsilon-greedy-methods",
    "title": "Multi-armed Bandits",
    "section": "\\(\\epsilon\\)-Greedy Methods",
    "text": "\\(\\epsilon\\)-Greedy Methods\n\nA simple alternative to allow for some exploration is to behave greedily most of the time, but every once in a while, say with small probability \\(\\epsilon\\), instead select randomly from among all the \\(k = \\left| \\mathcal{A} \\right|\\) actions with equal probability, independently of the action-value estimates.\n\n\nWe call methods using this near-greedy action selection rule \\(\\epsilon\\)-greedy methods.\n\n\nThe \\(\\epsilon\\)-greedy action selection method is defined as follows:\n\nWith probability \\(1-\\epsilon\\), select \\(A_t = \\text{argmax}_a Q_t(a)\\) (greedy action)\nWith probability \\(\\epsilon\\), select \\(A_t\\) randomly from \\(\\mathcal{A}\\)\n\n\n\n\\(\\implies P(A_t = a) = 1 - \\epsilon + \\frac{\\epsilon}{k}\\ \\)",
    "crumbs": [
      "Reinforcement Learning",
      "Multi-armed Bandits"
    ]
  },
  {
    "objectID": "pages/RL/Multi-armed-bandits.html#softmax-action-selection",
    "href": "pages/RL/Multi-armed-bandits.html#softmax-action-selection",
    "title": "Multi-armed Bandits",
    "section": "Softmax Action Selection",
    "text": "Softmax Action Selection\n\nThe softmax action selection rule is a “soft” version of the greedy action selection rule.\n\n\nIn \\(\\epsilon\\)-greedy methods, the greedy action gets the most probability mass, and all the other actions have an equal probability of being selected in the exploration phase.\n\n\nIf some actions are worse than others, should try to reduce the probability of selecting them during exploration.\n\n\nThe softmax action selection method can be denoted as\n\n\n\\[ P(A_t = a) = \\frac{e^{Q_t(a)/\\tau}}{\\sum_{b=1}^{k}e^{Q_t(b)/\\tau}} \\]\n\n\nwhere \\(\\tau\\) is the temperature parameter that controls the level of exploration. It can be “cooled” over time to reduce exploration.",
    "crumbs": [
      "Reinforcement Learning",
      "Multi-armed Bandits"
    ]
  },
  {
    "objectID": "pages/RL/Multi-armed-bandits.html#incremental-implementation",
    "href": "pages/RL/Multi-armed-bandits.html#incremental-implementation",
    "title": "Multi-armed Bandits",
    "section": "Incremental Implementation",
    "text": "Incremental Implementation\n\nThe action-value methods discussed so far all estimate action values as sample averages of observed rewards.\n\n\nTo simplify notation, we focus on a single action. Let \\(R_i\\) now denote the reward received after the \\(i\\)th selection of this action, and let \\(Q_n\\) denote the estimate of its action value after it has been selected \\(n-1\\) times, which we can now write simply as\n\n\n\\[ Q_{n} \\doteq \\frac{R_{1} + R_{2} + \\ldots + R_{n-1}}{n-1} \\]\n\n\nThe obvious implementation would be to maintain a record of all the rewards and then perform this computation whenever the estimated value was needed. However, if all the rewards are stored, as more rewards are seen, the memory and computational requirements would keep increasing.",
    "crumbs": [
      "Reinforcement Learning",
      "Multi-armed Bandits"
    ]
  },
  {
    "objectID": "pages/RL/Multi-armed-bandits.html#section-1",
    "href": "pages/RL/Multi-armed-bandits.html#section-1",
    "title": "Multi-armed Bandits",
    "section": "",
    "text": "These averages can be computed in a computationally efficient manner, in particular, with constant memory and constant per-time-step computation. Given \\(Q_{n}\\) and the \\(n\\)th reward, \\(R_{n}\\), the new average of all \\(n\\) rewards can be computed by\n\n\n\\(\\begin{aligned}\nQ_{n+1} & =\\frac{1}{n} \\ \\sum\\limits_{i=1}^{n} R_{i}\\\\\n& =\\frac{1}{n} \\ \\left( R_{n} +\\sum\\limits_{i=1}^{n-1} R_{i}\\right)\\\\\n& =\\frac{1}{n} \\ \\left( R_{n} +( n-1)\\frac{1}{n-1} \\ \\sum\\limits _{i=1}^{n-1} R_{i}\\right)\\\\\n& =\\frac{1}{n} \\ ( R_{n} +( n-1) Q_{n})\\\\\n& =\\frac{1}{n} \\ ( R_{n} +nQ_{n} -Q_{n})\\\\\n& =Q_{n} +\\frac{1}{n}[ R_{n} -Q_{n}]\n\\end{aligned}\\)\n\n\nwhich holds even for \\(n=1\\), obtaining \\(Q_{2} = R_{1}\\) for arbitrary \\(Q_{1}\\).",
    "crumbs": [
      "Reinforcement Learning",
      "Multi-armed Bandits"
    ]
  },
  {
    "objectID": "pages/RL/Multi-armed-bandits.html#section-2",
    "href": "pages/RL/Multi-armed-bandits.html#section-2",
    "title": "Multi-armed Bandits",
    "section": "",
    "text": "This update rule has the form of a stochastic averaging equation. The general form of such an equation is\n\n\n\\[ NewEstimate \\leftarrow OldEstimate + StepSize \\left[ Target - OldEstimate \\right] \\]\n\n\nThe expression \\([Target - OldEstimate]\\) is an error in the estimate. It is reduced by taking a step toward the “Target”.\n\n\nThe step-size parameter \\((StepSize)\\) used in the incremental implementation changes from time step to time step.\n\n\nIn processing the \\(n\\)th reward for an action, the step-size parameter is \\(\\frac{1}{n} \\\\\\). It is often denoted by \\(\\alpha\\) or, more generally, by \\(\\alpha_{t}(a)\\).",
    "crumbs": [
      "Reinforcement Learning",
      "Multi-armed Bandits"
    ]
  },
  {
    "objectID": "pages/RL/Multi-armed-bandits.html#nonstationary-problems",
    "href": "pages/RL/Multi-armed-bandits.html#nonstationary-problems",
    "title": "Multi-armed Bandits",
    "section": "Nonstationary Problems",
    "text": "Nonstationary Problems\n\nThe methods discussed so far are appropriate for stationary bandit problems, where the reward probabilities do not change over time.\n\n\nIn nonstationary cases, it makes sense to give more weight to recent rewards than to long-past rewards.\n\n\nOne way to do this is to use a constant step-size parameter. The incremental update rule for updating an average \\(Q_{n}\\) of the \\(n-1\\) past rewards is modified to be\n\n\n\\[ Q_{n+1} = Q_{n} + \\alpha \\left[ R_{n} - Q_{n} \\right] \\]\n\n\nwhere the step-size parameter \\(\\alpha \\in (0, 1]\\) is constant.",
    "crumbs": [
      "Reinforcement Learning",
      "Multi-armed Bandits"
    ]
  },
  {
    "objectID": "pages/RL/Multi-armed-bandits.html#section-3",
    "href": "pages/RL/Multi-armed-bandits.html#section-3",
    "title": "Multi-armed Bandits",
    "section": "",
    "text": "This results in \\(Q_{n+1}\\) being a weighted average of past rewards and the initial estimate \\(Q_{1}\\):\n\n\n\\(\\begin{array}{ r c l }\nQ_{n+1} & = & Q_{n} +\\alpha [R_{n} -Q_{n} ]\\\\\n& = & \\alpha R_{n} +(1-\\alpha )Q_{n}\\\\\n& = & \\alpha R_{n} +(1-\\alpha )[\\alpha R_{n-1} +(1-\\alpha )Q_{n-1} ]\\\\\n& = & \\alpha R_{n} +(1-\\alpha )\\alpha R_{n-1} +(1-\\alpha )^{2} Q_{n-1}\\\\\n& = & \\alpha R_{n} +(1-\\alpha )\\alpha R_{n-1} +(1-\\alpha )^{2} \\alpha R_{n-2} +\\\\\n&  & \\dotsc +(1-\\alpha )^{n-1} \\alpha R_{1} +(1-\\alpha )^{n} Q_{1}\\\\\n& = & \\ (1-\\alpha )^{n} Q_{1} +\\sum\\limits _{i=1}^{n} \\alpha (1-\\alpha )^{n-i} R_{i}\n\\end{array}\\)",
    "crumbs": [
      "Reinforcement Learning",
      "Multi-armed Bandits"
    ]
  },
  {
    "objectID": "pages/RL/Multi-armed-bandits.html#section-4",
    "href": "pages/RL/Multi-armed-bandits.html#section-4",
    "title": "Multi-armed Bandits",
    "section": "",
    "text": "We call this a weighted average because the sum of the weights is \\((1-\\alpha )^{n} +\\sum\\limits _{i=1}^{n} \\alpha (1-\\alpha )^{n-i} = 1\\).\n\n\nNote that the weight \\(\\alpha (1-\\alpha )^{n-i}\\), given to the reward \\(R_{i}\\), depends on how many rewards ago, \\(n-i\\), it was observed.\n\n\nThe quantity \\(1-\\alpha\\) is less than \\(1\\), so the weight given to \\(R_i\\) decreases as the number of intervening rewards increases.\n\n\nIn fact, the weight decays exponentially according to the exponent on \\(1-\\alpha\\). If \\(1-\\alpha = 0\\), then all the weight goes on the very last reward, \\(R_n\\), because of the convention that \\(0^0 = 1\\).\n\n\nAccordingly, this is sometimes called an exponential recency-weighted average.",
    "crumbs": [
      "Reinforcement Learning",
      "Multi-armed Bandits"
    ]
  },
  {
    "objectID": "pages/RL/Multi-armed-bandits.html#upper-confidence-bound-ucb-action-selection",
    "href": "pages/RL/Multi-armed-bandits.html#upper-confidence-bound-ucb-action-selection",
    "title": "Multi-armed Bandits",
    "section": "Upper-Confidence-Bound (UCB) Action Selection",
    "text": "Upper-Confidence-Bound (UCB) Action Selection",
    "crumbs": [
      "Reinforcement Learning",
      "Multi-armed Bandits"
    ]
  },
  {
    "objectID": "pages/random/Vectors-in-alternate-basis.html",
    "href": "pages/random/Vectors-in-alternate-basis.html",
    "title": "Arachnidly",
    "section": "",
    "text": "ReuseCC BY-NC-SA 4.0",
    "crumbs": [
      "Random Math",
      "Vectors in Alternate Basis"
    ]
  },
  {
    "objectID": "originals/index.html",
    "href": "originals/index.html",
    "title": "Original Works",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nThe Misunderstood Concept of Etiquette in Indian English\n\n\n\n\n\n\narticle\n\n\nindian-culture\n\n\nlanguage\n\n\ncommunication\n\n\n\n\n\n\n\n\n\nAug 4, 2024\n\n\nAra C\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Original Works"
    ]
  },
  {
    "objectID": "originals/articles/Etiquette-In-Indian-English/index.html",
    "href": "originals/articles/Etiquette-In-Indian-English/index.html",
    "title": "The Misunderstood Concept of Etiquette in Indian English",
    "section": "",
    "text": "Etiquette, a concept deeply embedded in the norms of polite behaviour and social conduct, generally assumes very peculiar manifestations when it gets adapted to the setting of different cultures. The way Indians have interpreted etiquette in the English language sometimes differs from traditional Western understanding. This article will draw on the specifics of this change, helping to illustrate how Indian English etiquette in and of itself is a reflection of both cultural diversity and historical influence.\nHistorical Context and Colonial Influence\nThe understanding of etiquette in India has been radically structured through its colonial history. Due to British dominion, English was cultivated as the language of administration and education, and so too were Western notions of etiquette naturalised into the culture. However, these ideas were adapted to the local context. For instance, the British would teach formal greetings and rigid table manners; Indian versions included aspects of their tradition such as foot touching to show reverence or dining as a group. These integrations of culture showed how colonial influences found ways into India’s pre-established culture, which uniquely in that process developed a version of etiquette that would be characteristically Indian (Tinkham, 1993).\nPoliteness and Hierarchical Respect\nPoliteness in Indian English is richly contributed to by the respect accorded to hierarchy and age. An elder, even informally, would always be addressed as “Sir” or “Madam.” Similarly, this kind of respect can also be seen at workplaces, where everybody from top to bottom is properly treated and maintained with a level of formality. On the other hand, Western etiquette may be more concerned with principles of equality and, at times, bluntness, which can become unresolvable conflicts when these cultural norms intersect. For example, in their research, Tinkham points out that Westernised Indians use phrases such as “thank you” and “I’m sorry” more than do the less Westernised ones. Clearly, there appears to be a need for understanding the cultural context in which language is used. It plays a very important role in communication and interactions between social beings.\nCommunication Styles\nIndian English etiquette can also be seen in communication styles. Indirectness is often preferred due to the desire not to face or cause confrontation and disharmony. Polite methods that a person might refuse include “I will try” or “Let’s see,” with such phrases expressing refusal or unverifiability. In Western society, where directness is respected, this would be considered evasiveness. This tendency towards ambiguity may give rise to misinterpretations, in which politeness in one culture seems ambiguous to others (Kurchenkova, Palashevskaya, & Kurchenkova, 2019). Understanding these differences is essential to effective cross-cultural communication, as it leads to a lack of confusion and gives room for a more respectful and empathetic interaction.\nHospitality and Guest Relations\nHospitality stands as a foundation of Indian culture, which can be mirrored by Indian English etiquette. Guests are considered with due respect and care, often with elaborate displays of hospitality. More than common polite invitations, sentences like “Please come,” and “You must have tea” reveal firm expectations. Western etiquette, on the other hand, might see such insistence as too much on the guest, which goes to show that there is a cultural difference in hosting and guest relations. This focus on hospitality really exemplifies the Indian value of treating guests as gods, a concept that dates back to the traditional saying “Atithi Devo Bhava.”\nThe Role of Apologies\nApologies in Indian English etiquette have a much more discreet role than in Western etiquette. The tradition of saying “sorry” often, even for quite small inconveniences, is a strategy to maintain social harmony and show care towards others. In Western cultures, apologies tend to be more sparing, and they are often reserved to show awareness of important mistakes. This may sometimes lead to confusion where frequent apologies indicate the weakness of a person in certain respects, not so much politeness (Bharuthram, 2003). It is for this very reason that the difference in culture with regard to behaviour such as an apology has great significance placed on context and indicates the possibility of cross-cultural misunderstanding in relation to the particular aspect of concern.\nAdapting to Global Standards\nThe more India integrates with the global community, there is an acute awareness of the need to balance traditional etiquette with global standards. This will hopefully allow for a more direct approach to workplace communication, whereby a better understanding is gained regarding the subtleties of Western manners. Equally as important are the efforts that need to be put in place to preserve those qualities of Indian etiquette that bring value and richness to interpersonal encounters and best reflect the cultural heritage of this country. It is in negotiating this fine balance that Indians who operate in a global environment ensure that they retain their cultural identity.\nConclusion\nIndian English etiquette is an interesting juxtaposition of traditional Indian values and colonial influences, adapted to contemporary life. Even as it might contrast with Western notions of politeness, there is deep respect, hospitality, and harmony in it. Understanding such differences is crucial for improving interaction and mutual respect between people in the globalising world. By appreciating unique expressions of etiquette across cultures, we are in a much better position to navigate social interactions with greater awareness.\n\n\n\nReuseCC BY-NC-SA 4.0"
  },
  {
    "objectID": "pages/Stats/Probability-Basics.html",
    "href": "pages/Stats/Probability-Basics.html",
    "title": "Basic Concepts of Probability",
    "section": "",
    "text": "Experiment\n\nProcess or phenomenon that we wish to study statistically\n\n\n\nOutcome\n\nResult of the experiment (in as much detail as necessary)",
    "crumbs": [
      "Statistics",
      "Basic Concepts of Probability"
    ]
  },
  {
    "objectID": "pages/Stats/Probability-Basics.html#experiment-and-outcome",
    "href": "pages/Stats/Probability-Basics.html#experiment-and-outcome",
    "title": "Basic Concepts of Probability",
    "section": "",
    "text": "Experiment\n\nProcess or phenomenon that we wish to study statistically\n\n\n\nOutcome\n\nResult of the experiment (in as much detail as necessary)",
    "crumbs": [
      "Statistics",
      "Basic Concepts of Probability"
    ]
  },
  {
    "objectID": "pages/Stats/Probability-Basics.html#sample-space",
    "href": "pages/Stats/Probability-Basics.html#sample-space",
    "title": "Basic Concepts of Probability",
    "section": "Sample Space",
    "text": "Sample Space\nDefinition: A sample space S is a set that contains all possible outcomes of an experiment.\n\nSample space is a set, typically denoted S\nExamples:\n\nFlipping a coin: S = \\{\\text{Heads}, \\text{Tails}\\}\nRolling a die: S = \\{1, 2, 3, 4, 5, 6\\}",
    "crumbs": [
      "Statistics",
      "Basic Concepts of Probability"
    ]
  },
  {
    "objectID": "pages/Stats/Probability-Basics.html#event",
    "href": "pages/Stats/Probability-Basics.html#event",
    "title": "Basic Concepts of Probability",
    "section": "Event",
    "text": "Event\nDefinition: An event is a subset of the sample space.\n\nExamples:\n\nToss a coin: S = \\{\\text{Heads}, \\text{Tails}\\}\n\nEvents: \\emptyset, \\{\\text{Heads}\\}, \\{\\text{Tails}\\}, \\{\\text{Heads}, \\text{Tails}\\}\n4 events\n\nThrow a die: S = \\{1, 2, 3, 4, 5, 6\\}\n\nEvents: \\emptyset, \\{1\\}, \\{2\\}, \\{3\\}, \\{4\\}, \\{5\\}, \\{6\\}, \\{1, 2\\}, \\{1, 3\\}, \\ldots, \\{1, 2, 3, 4, 5, 6\\}\n64 events\n\n\nAn event is said to have “occurred” if the actual outcome of the experiment belongs to the event.\nEvents are sets\n\nAll set theory notions apply to events\n\nOne event can be contained in another, i.e. A \\subseteq B\n\nThrow a die: A = \\{2, 6\\}, B = \\text{Even number}\nIf A occured, B has also occured\nIf B occured, A may or may not have occured\n\nComplement of an event A, denoted A^c = \\{\\text{outcomes in } S \\text{ not in } A\\} = (S \\setminus A)\n\nThrow a die: A = \\{2, 4, 6\\} (even), A^c = \\{1, 3, 5\\} (odd)\nIf A occured, A^c did not occur\nIf A^c occured, A did not occur\n\n\n\nCombining events to create new events\n\nUnion of events A and B, denoted A \\cup B = \\{x: x \\in A \\text{ or } x \\in B\\}\n\nThrow a die: A = \\{1, 2, 3\\}, B = \\{3, 4, 5\\}, A \\cup B = \\{1, 2, 3, 4, 5\\}\n\nIntersection of events A and B, denoted A \\cap B = \\{x: x \\in A \\text{ and } x \\in B\\}\n\nThrow a die: A = \\{1, 2, 3\\}, B = \\{3, 4, 5\\}, A \\cap B = \\{3\\}\n\n\n\n\nDisjoint events\n\nTwo events with an empty intersection are said to be disjoint or mutually exclusive events\n\nThrow a die: even number, odd number are disjoint\n\nSuppose A and B are disjoint events\n\nIf A occured, B did not occur\nIf B occured, A did not occur\n\nEvent and its complement\n\nA and A^c are disjoint, i.e. A \\cap A^c = \\emptyset\nTogether, they cover the entire sample space, i.e. A \\cup A^c = S\n\nEither A or A^c must occur\n\nA and A^c are an example of a partition of the sample space\n\nPartition: Collection of disjoint sets that together cover the entire sample space\n\n\nMultiple events: E_1, E_2, \\ldots, E_n are disjoint if, for any i \\neq j, E_i \\cap E_j = \\emptyset",
    "crumbs": [
      "Statistics",
      "Basic Concepts of Probability"
    ]
  },
  {
    "objectID": "pages/Stats/Probability-Basics.html#probability",
    "href": "pages/Stats/Probability-Basics.html#probability",
    "title": "Basic Concepts of Probability",
    "section": "Probability",
    "text": "Probability\nDefinition: “Probability” is a function P that assigns to each event a real number between 0 and 1. The entire probability space (sample space, events and probability function) should satisfy the following two axioms:\n\nP(S) = 1 (probability of the entire sample space equals 1)\nIf E_1, E_2, E_3, \\ldots are disjoint events (how many events? Could be infinitely many), P(E_1 \\cup E_2 \\cup E_3 \\cup \\ldots) = P(E_1) + P(E_2) + P(E_3) + \\ldots",
    "crumbs": [
      "Statistics",
      "Basic Concepts of Probability"
    ]
  },
  {
    "objectID": "pages/Stats/Probability-Basics.html#basic-properties-of-probability",
    "href": "pages/Stats/Probability-Basics.html#basic-properties-of-probability",
    "title": "Basic Concepts of Probability",
    "section": "Basic Properties of Probability",
    "text": "Basic Properties of Probability\n\nTheorem 1 (Basic Properties of Probability) Let P be a probability on a sample space S. Then,\n\nP(\\emptyset) = 0\nLet E^c be the complement of Event E. Then, \\newline P(E^c) = 1 - P(E)\nIf Event E is a subset of Event F, i.e. E \\subseteq F, then \\newline P(F) = P(E) + P(F \\setminus E)\\text{,} which implies that P(E) \\leq P(F)\n3.a. If E and F are events, then P(E) = P(E \\cap F) + P(E \\setminus F) \\text{,} P(F) = P(E \\cap F) + P(F \\setminus E)\nIf E and F are events, then P(E \\cup F) = P(E) + P(F) - P(E \\cap F)\n\n\n\nProof. \n\n\n\\emptyset^c = S and \\emptyset, S are disjoint and \\emptyset \\cup S = S.\nBy Axiom 2, P(\\emptyset \\cup S) = P(\\emptyset) + P(S) or P(S) = P(\\emptyset) + P(S) or P(\\emptyset) = 0.\n\n\nE and E^c are disjoint and E \\cup E^c = S.\nBy Axiom 2, P(E \\cup E^c) = P(E) + P(E^c) or P(S) = P(E) + P(E^c)\nBy Axiom 1, 1 = P(S) = P(E) + P(E^c)\nSo, P(E^c) = 1 - P(E)\n\n\nF \\setminus E = F \\cap E^c (outside of E and inside of F)\nE and F \\setminus E are disjoint and E \\cup (F \\setminus E) = F\nBy Axiom 2, P(E \\cup (F \\setminus E)) = P(E) + P(F \\setminus E) or P(F) = P(E) + P(F \\setminus E)\n\n3.a.\n\nE \\cap F is a subset of E\nBy subset property, P(E) = P(E \\cap F) + P(E \\setminus (E \\cap F))\nNow, E \\setminus (E \\cap F) = E \\setminus F\nSo, P(E) = P(E \\cap F) + P(E \\setminus F)\n\n\nE \\cup F = (E \\setminus F) \\cup (E \\cap F) \\cup (F \\setminus E) and the 3 events on RHS are disjoint\nP(E \\cup F) = P(E \\setminus F) + P(E \\cap F) + P(F \\setminus E)\nUse P(E \\setminus F) = P(E) - P(E \\cap F) and P(F \\setminus E) = P(F) - P(E \\cap F)",
    "crumbs": [
      "Statistics",
      "Basic Concepts of Probability"
    ]
  },
  {
    "objectID": "pages/Stats/Probability-Basics.html#conditional-probability-space",
    "href": "pages/Stats/Probability-Basics.html#conditional-probability-space",
    "title": "Basic Concepts of Probability",
    "section": "Conditional Probability Space",
    "text": "Conditional Probability Space\nDefinition: Consider a probability space: Sample space S, collection of events, and a probability function P. Let B be an event with P(B) &gt; 0.\n\\newlineSample space: B \\newlineEvents: A \\cap B for every event A in the original space\n\\text{Probability function: } \\frac{P(A \\cap B)}{P(B)}\n\\newline(denoted P(A|B) and called conditional probability of A given B)\nFor any event A in the original space, P(A \\cap B) = P(B) \\cdot P(A|B)",
    "crumbs": [
      "Statistics",
      "Basic Concepts of Probability"
    ]
  },
  {
    "objectID": "pages/Stats/Probability-Basics.html#law-of-total-probability",
    "href": "pages/Stats/Probability-Basics.html#law-of-total-probability",
    "title": "Basic Concepts of Probability",
    "section": "Law of Total Probability",
    "text": "Law of Total Probability\n\nTheorem 2 (Law of Total Probability) B_1, B_2, \\ldots \\text{ : Partition of } S  P(A) = P(A \\cap B_1) + P(A \\cap B_2) + \\ldots = P(A|B_1) P(B_1) + P(A|B_2) P(B_2) + \\ldots\n\n\nProof. \n\nA: disjoint union of A \\cap B_1, A \\cap B_2, \\ldots\nBy Axiom 2, P(A) = P(A \\cap B_1) + P(A \\cap B_2) + \\ldots\nUsing conditional probability on each term above, we get the result",
    "crumbs": [
      "Statistics",
      "Basic Concepts of Probability"
    ]
  },
  {
    "objectID": "pages/Stats/Probability-Basics.html#bayes-theorem",
    "href": "pages/Stats/Probability-Basics.html#bayes-theorem",
    "title": "Basic Concepts of Probability",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\n\nTheorem 3 (Bayes’ Theorem)  A,B \\text{: Events with } P(A) &gt; 0 \\text{, } P(B) &gt; 0 P(A \\cap B) = P(B)P(A|B) = P(A)P(B|A) P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n\n\nProof. \n\nBayes’ Theorem may be derived from the definition of conditional probability: P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\text{, if } P(B) \\neq 0,\nwhere P(A \\cap B) is the probability of both A and B being true. Similarly, P(B|A) = \\frac{P(A \\cap B)}{P(A)} \\text{, if } P(A) \\neq 0.\nSolving for P(A \\cap B) and substituting into the above expression for P(A|B) yields Bayes’ Theorem: P(A|B) = \\frac{P(B|A)P(A)}{P(B)} \\text{, if } P(B) \\neq 0.",
    "crumbs": [
      "Statistics",
      "Basic Concepts of Probability"
    ]
  },
  {
    "objectID": "pages/Stats/Probability-Basics.html#independence-of-events",
    "href": "pages/Stats/Probability-Basics.html#independence-of-events",
    "title": "Basic Concepts of Probability",
    "section": "Independence of Events",
    "text": "Independence of Events\nDefinition: Two events A and B are independent if P(A \\cap B) = P(A)P(B)\n\nIf P(B) &gt; 0, P(A|B) = P(A)\nDisjoint events are never independent\nFor events to be independent, they should have a non-empty intersection",
    "crumbs": [
      "Statistics",
      "Basic Concepts of Probability"
    ]
  },
  {
    "objectID": "pages/Stats/Probability-Basics.html#mutual-independence-of-multiple-events",
    "href": "pages/Stats/Probability-Basics.html#mutual-independence-of-multiple-events",
    "title": "Basic Concepts of Probability",
    "section": "Mutual independence of multiple events",
    "text": "Mutual independence of multiple events\nDefinition: Events A_1, A_2, \\ldots, A_n are mutually independent if, for all i_1, i_2, \\ldots, i_k, P(A_{i_1} \\cap A_{i_2} \\cap \\ldots \\cap A_{i_k}) = P(A_{i_1})P(A_{i_2}) \\ldots P(A_{i_k})\n\nA and B are independent \\implies A and B^c are independent\n\nP(A \\cap B^c) = P(A \\setminus B) = P(A) - P(A \\cap B) = P(A)(1 - P(B)) = P(A)P(B^c)\nIntuitive: B does not affect A \\implies B^c does not affect A\nTwo events are independent \\implies Complement of one event is independent of the other\n\nUsing the above twice: A and B are independent \\implies A^c and B^c are independent\nExtension: n events are mutually independent \\implies any subset with or without complementing are independent as well",
    "crumbs": [
      "Statistics",
      "Basic Concepts of Probability"
    ]
  },
  {
    "objectID": "pages/random/Fourier.html",
    "href": "pages/random/Fourier.html",
    "title": "Arachnidly",
    "section": "",
    "text": "Some handwritten notes on Fourier Series Representations\n\n\n\n\nReuseCC BY-NC-SA 4.0",
    "crumbs": [
      "Random Math",
      "Fourier Series Representations"
    ]
  }
]